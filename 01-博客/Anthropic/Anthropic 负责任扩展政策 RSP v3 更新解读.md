---
title: "Anthropic 负责任扩展政策 RSP v3 更新解读"
作者: "Anthropic"
来源: "www.anthropic.com"
原文链接: "https://www.anthropic.com/news/responsible-scaling-policy-v3"
发布日期: "2026-02-25"
译注: "本文基于原文翻译整理，保留关键术语英文。"
---

## 摘要

**1) 一句话摘要**
Anthropic 发布了第三版“负责任的扩展政策”（RSP v3），基于过去两年的实践经验，新版政策将公司单方面可执行的安全承诺与全行业多边协作建议区分开来，并引入了《前沿安全路线图》和每 3-6 个月发布一次的风险报告机制以提升透明度。

**2) 关键要点**
*   **政策背景与演进：** RSP 是基于“如果-那么”条件承诺的自愿性框架（即 AI 安全级别 ASL）。由于早期发现 ASL-4 及以上级别的安全保障难以准确定义，v3 版本对其进行了务实的重组。
*   **既往成效：** 成功推动了内部安全保障的开发（如针对 ASL-3 的化学和生物武器风险分类器），促使 OpenAI 和 Google DeepMind 采用类似框架，并为加州 SB 53、纽约州 RAISE 及《欧盟 AI 法案》等早期 AI 政策提供了参考。
*   **区分单边与多边行动：** 新版 RSP 明确划分了两套措施：Anthropic 无论外部环境如何都将单方面推行的缓解措施，以及需要全行业共同实施的“能力-缓解措施”映射图。
*   **发布《前沿安全路线图》：** 设定公开但不具约束力的目标，包括信息安全领域的“登月级研发”、超越传统漏洞赏金计划的高级红队测试、确保模型遵守宪法的系统性措施，以及集中记录和监控 AI 开发活动。
*   **定期发布风险报告：** 承诺每 3-6 个月发布一次系统性的风险报告（仅作最低限度删减），详细说明模型能力、威胁模型、主动风险缓解措施以及整体风险评估水平。
*   **引入外部审查机制：** 在特定情况下，将聘请无利益冲突的专家级第三方审查员，获取未删减的风险报告权限，对 Anthropic 的推理和决策进行全面的公开审查（目前正在试点）。

**3) 风险/差距**
*   **能力评估的“模糊地带”：** 预设的模型能力阈值在实践中难以清晰界定（如生物风险测试结果模棱两可），现有的模型评估科学尚不成熟，无法提供决定性的风险高低证明。
*   **单边防御的极限：** 针对更高能力水平（ASL-4 及以上，如防范国家级行为者的高级网络攻击），单一公司极难或无法单方面实现所需的安全标准（如 RAND 报告指出的 SL5 标准），必须依赖国家安全界和集体行动。
*   **政府行动滞后与政治气候：** 政策环境目前优先考虑 AI 竞争力和经济增长，导致政府在 AI 安全方面的多边协调和实质性行动进展缓慢。
*   **实践与理想的差距：** 未来的风险报告将明确披露 Anthropic 当前实际采取的安全/安保措施与针对全行业提出的更雄心勃勃的安全建议之间存在的差距。

## 正文

我们正在发布第三版“负责任的扩展政策”（Responsible Scaling Policy，简称 RSP），这是我们用于缓解 AI 系统灾难性风险的自愿性框架。

Anthropic 实施 RSP 至今已有两年多，我们对其优势和不足有了深刻的认识。因此，我们正在更新该政策，以巩固迄今为止行之有效的措施，在必要时改进政策，并实施新举措以提高我们决策的透明度和问责制。

您可以在此处阅读新版 RSP 的全文。在本文中，我们将探讨这些变更背后的一些思考。

## 最初的 RSP 与我们的变革理论

RSP 是我们试图解决的一个难题：如何应对在编写政策时尚未出现，但可能会随着技术呈指数级发展而迅速涌现的 AI 风险。当我们在 2023 年 9 月编写最初的 RSP 时，大型语言模型本质上还只是聊天界面。如今，它们已经能够浏览网页、编写和运行代码、使用计算机，并采取自主的多步行动。随着这些新能力的出现，新的风险也随之而来。我们预计这种模式将持续下去。

我们将 RSP 的重点放在了*有条件*（或*如果-那么*）的承诺原则上。*如果*一个模型超过了特定的能力水平（例如，能够协助制造危险武器的生物科学能力），*那么*政策规定我们应该引入一套全新且更严格的安全保障措施（例如，防止模型被滥用和模型权重被盗）。

每套安全保障措施都对应一个“AI 安全级别”（AI Safety Level，简称 ASL）：例如，ASL-2 指代一套必需的安全保障措施，而 ASL-3 则指代能力更强的 AI 模型所需的一套更严格的安全保障措施。

早期的 ASL（ASL-2 和 ASL-3）有着非常详细的定义，但要为距离问世还有好几代的模型指定正确的安全保障措施则困难得多。因此，我们有意将后期的 ASL（ASL-4 及以上）在很大程度上留作未定义状态，并希望在我们对更高的 AI 能力水平会有何影响有了更清晰的认识后，再对其进行更详细的制定。

以下是我们“变革理论”的粗略描述——即我们希望通过 RSP 影响生态系统的机制：

- **内部强制机制。** 在 Anthropic 内部，我们希望 RSP 能够迫使我们将重要的安全保障措施作为发布（和训练）新模型的先决条件。这向不断壮大的庞大组织明确了这些安全保障措施的重要性，从而激励我们加快进展。

- **竞优（Race to the top）。** 我们希望发布 RSP 能够鼓励其他 AI 公司出台类似的政策。这就是“竞优”（与“竞底”相反）的理念，即激励不同的行业参与者去改善，而不是削弱其模型的安全保障措施和整体安全态势。随着时间的推移，我们希望 RSP 或类似政策能够成为自愿性的行业标准，或者进一步为旨在鼓励 AI 模型开发安全性和透明度的 AI 法律提供参考。

- **就风险达成更多共识。** 我们将能力阈值视为对行业具有潜在重要意义的时刻。如果我们达到了一个重要的能力阈值（例如 AI 模型支持端到端制造生物武器的能力），我们将自行制定相应的安全保障措施，并利用我们获得的关于 AI 能力的证据，倡导其他公司和政府也采取行动。换言之，我们认为能力阈值可能是超越单边行动（Anthropic 要求对其自身模型采取安全保障措施）并鼓励多边行动（其他 AI 公司和/或政府也要求采取此类安全保障措施）的良好契机。

- **展望未来。** 我们认识到，在后期的一些能力阈值上，我们设想的应对措施强度（例如，实现针对国家级行为者滥用 AI 模型的高鲁棒性）很可能是 Anthropic 难以或无法单方面完成的。我们希望，当我们达到这些更高的能力水平时，世界能够清楚地看到其中的危险，并且我们能够与全球各国政府协调，共同实施一家公司难以单独完成的安全保障措施。

## 评估我们的变革理论

两年半后，我们诚实的评估是，这一变革理论的某些部分如我们所愿发挥了作用，但其他部分则不然。以下是 RSP 取得成功的领域：

- 我们的 RSP 确实激励了我们开发更强大的安全保障措施。例如，为了符合我们的 ASL-3 部署标准（该标准主要针对资源和专业知识相对有限的威胁行为者所带来的化学和生物武器风险），我们开发了越来越复杂和准确的方法（具体而言，即输入和输出分类器）来拦截令人担忧的内容。

- 更广泛地说，ASL-3 标准的整体实施确实被证明是可行的。我们在 2025 年 5 月为相关模型激活了 ASL-3 安全保障措施，并从那时起一直致力于改进它们。

- 我们的 RSP 确实鼓励了其他 AI 公司采用有些类似的标准：在宣布我们的 RSP 后的几个月内，OpenAI 和 Google DeepMind 都采用了大致相似的框架。一些公司也实施了与我们的 ASL-3 防御机制类似的生物武器相关分类器。这些自愿性标准（包括 RSP 中的标准）背后的原则，为早期 AI 政策的制定提供了参考。我们已经看到世界各地的政府（例如加利福尼亚州的 SB 53 法案、纽约州的 RAISE 法案，以及《欧盟 AI 法案》的实践准则）开始要求前沿 AI 开发者创建并发布用于评估和管理灾难性风险的框架——Anthropic 通过包括其《前沿合规框架》在内的公开文档来满足这些要求。鼓励行业建立这种严格的透明度框架，正是我们 RSP 的初衷。

然而，我们变革理论的其他部分并没有如我们所愿地实现：

- 利用 RSP 阈值就 AI 风险达成更多共识的想法在实践中并未完全奏效——尽管产生了一些效果。我们发现预设的能力水平比我们预期的要模糊得多：在某些情况下，模型能力显然已经接近 RSP 阈值，但对于它们是否已经明确越过这些阈值，我们存在极大的不确定性。模型评估的科学还不够成熟，无法提供决定性的答案。在这些情况下，我们采取了预防性方法并实施了相关的安全保障措施，但我们内部的不确定性转化为在整个 AI 行业采取多边行动时缺乏有力的外部理由。生物风险就是这种“模糊地带”的一个例子。我们的模型现在展示出足够的生物学知识，能够通过我们能够快速、轻松运行的大多数测试，因此我们不再能有力地论证特定模型的风险很低。但仅凭这些测试也不足以有力地论证风险很*高*。我们一直在寻找更多证据，例如支持一项广泛的湿实验室试验，但结果仍然模棱两可，特别是因为这些研究耗时较长，等到完成时，更强大的模型已经问世了。

- 生物风险就是这种“模糊地带”的一个例子。我们的模型现在展示出足够的生物学知识，能够通过我们能够快速、轻松运行的大多数测试，因此我们不再能有力地论证特定模型的风险很低。但仅凭这些测试也不足以有力地论证风险很*高*。我们一直在寻找更多证据，例如支持一项广泛的湿实验室试验，但结果仍然模棱两可，特别是因为这些研究耗时较长，等到完成时，更强大的模型已经问世了。*(注：此处保留了原文中重复出现的段落)*

- 尽管过去三年 AI 能力取得了快速进步，但政府在 AI 安全方面的行动却进展缓慢。政策环境已转向优先考虑 AI 竞争力和经济增长，而以安全为导向的讨论尚未在联邦层面获得实质性的推动力。我们仍然坚信，政府在 AI 安全方面的有效参与既是必要的，也是可行的，我们的目标是继续推进基于证据、国家安全利益、经济竞争力和公众信任的对话。但这被证明是一个长期项目——而不是随着 AI 能力增强或跨越某些阈值就会自然发生的事情。

如上所述，我们能够单方面实施 ASL-3 安全保障措施，且对公司运营的成本在合理范围内。然而，对于更高的能力水平和更高的 ASL，情况可能不再如此。虽然我们更高的 ASL 在很大程度上未被定义，但我们在之前的 RSP 中制定的强有力的缓解措施，如果没有集体行动，可能会被证明完全无法实施。为了说明这一挑战的规模，兰德公司（RAND）一份关于模型权重安全的报告指出，其旨在阻止最具网络能力的机构进行最高优先级操作的“SL5”安全标准“目前是不可能实现的”，并且“可能需要国家安全界的协助”。

(a) 模糊地带混淆了公众对风险的认知，(b) 反监管的政治气候，以及 (c) 更高 RSP 级别的要求极难单方面满足，这三者的结合为我们当前的 RSP 带来了结构性挑战。我们本可以尝试通过将 ASL-4 和 ASL-5 的安全保障措施定义为易于合规的方式来解决这个问题——但这将破坏 RSP 的初衷精神。

相反，我们选择透明地承认这些挑战，并在达到这些更高水平之前重组 RSP。修订后的 RSP 旨在采取更现实的单边承诺，这些承诺虽然困难，但在当前环境下仍然是可以实现的，同时继续全面描绘我们认为整个行业需要通过多边方式应对的风险。

## 更新我们的负责任的扩展政策

我们新版 RSP 包含三个关键要素。

### 1. 将我们作为公司的计划与我们对行业的建议区分开来

我们的 RSP 现在概述了两套缓解措施：第一，无论其他方采取何种行动，我们都计划推行的缓解措施；第二，一份雄心勃勃的“能力-缓解措施”映射图，我们相信，如果在整个 AI 行业实施，将有助于充分管理高级 AI 带来的风险。

阅读完整的负责任的扩展政策。

### 2. 前沿安全路线图 (Frontier Safety Roadmap)

我们的新版 RSP 引入了一项要求，即制定并发布《前沿安全路线图》，该路线图将描述我们在安全（Security）、对齐（Alignment）、安全保障（Safeguards）和政策（Policy）领域缓解风险的具体计划。路线图中描述的目标旨在做到既雄心勃勃又切实可行——提供一种我们认为是过去 RSP 成功经验的内部强制机制。

这些并非硬性承诺，而是我们将公开对自身进展进行评分的公开目标。这种“不具约束力但公开宣布”目标的策略，借鉴了我们一直倡导的前沿 AI 立法的透明度方法（尽管它向公众提供的细节远超现有立法的要求），也借鉴了我们之前版本 RSP 的成功经验。

我们当前《前沿安全路线图》中的一些示例目标包括：

- 启动“登月级研发（moonshot R&D）”项目，探索雄心勃勃、可能打破常规的方法，以实现前所未有的信息安全水平；

- 开发一种对我们的系统进行红队测试的方法（可能涉及高度自动化），该方法要超越我们漏洞赏金计划中数百名参与者的集体贡献；

- 实施一系列系统性措施，以确保 Claude 按照其宪法行事；

- 建立我们所有关键 AI 开发活动的全面、集中的记录，并使用 AI 分析这些记录以发现问题，包括内部人员（人类和 AI）的令人担忧的行为以及安全威胁；

- 发布一份政策路线图，其中包含关于“监管阶梯”的具体提案——这些政策随风险的增加而升级，并有助于指导政府的 AI 政策。

阅读《前沿安全路线图》以了解有关这些目标及我们其他目标的更多信息。

### 3. 风险报告与外部审查

风险报告是我们改进之前 RSP 中行之有效措施的另一种方式。我们发现，制作一份原型风险报告（即我们 2025 年 5 月的《安全保障报告》）对我们内部理解和向公众传达风险非常有用。风险报告将此扩展为一种更系统、更全面的实践。

风险报告将提供发布时我们模型安全状况的详细信息。它们不仅会描述模型的能力，还会解释能力、威胁模型（模型可能构成威胁的具体方式）以及主动风险缓解措施是如何相互配合的，并提供对整体风险水平的评估。风险报告将每 3-6 个月在网上发布一次（会进行一些删减 1）。

新版 RSP 还要求在特定情况下对风险报告进行外部审查。我们将指定熟悉 AI 安全研究、有动力对 Anthropic 的安全立场保持公开诚实，且没有重大潜在利益冲突的专家级第三方审查员。他们将获得未删减或最低限度删减的风险报告访问权限，并将对我们的推理、分析和决策进行全面的公开审查。尽管我们目前的模型尚不需要外部审查，但我们已经在进行试点，并正朝着这个目标努力。

风险报告将指出我们当前的安全和安保措施与我们对全行业安全提出的更雄心勃勃的建议之间的任何差距。我们希望，描述并公开这些差距有助于提高公众意识，从而在未来促成有益的政策变革。

阅读初始风险报告。

## 结论

负责任的扩展政策一直被规划为一份动态文档：一项具有灵活性、能够随着 AI 模型能力增强而改变的政策。这第三次修订放大了之前 RSP 中行之有效的部分，承诺对我们的计划和风险考量保持更高的透明度，并将我们对整个行业的建议与我们作为一家独立公司所能实现的目标区分开来。

本着同样的务实精神，随着技术的发展，我们将继续修订和完善我们的 RSP，以及我们评估和缓解风险的方法。

1. 正如我们在 RSP 中所讨论的，我们将致力于尽量减少对风险报告公开版本的删减。尽管如此，我们可能仍需删减部分文本的原因包括法律合规、知识产权保护、公共安全和隐私。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/alignment]]
- [[00-元语/security]]
- [[00-元语/risk]]
