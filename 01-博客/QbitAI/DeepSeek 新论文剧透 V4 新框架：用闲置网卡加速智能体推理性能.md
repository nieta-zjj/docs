---
title: "DeepSeek 新论文剧透 V4 新框架：用闲置网卡加速智能体推理性能"
发布日期: 2026-02-27
来源: "QbitAI"
原文链接: "https://www.qbitai.com/2026/02/382872.html"
---

## 摘要

### 一句话总结
DeepSeek联合北大清华发布了DualPath推理框架，通过创新的双路径加载架构利用解码侧闲置网卡带宽，成功打破了智能体长文本推理的I/O瓶颈并大幅提升了系统吞吐量。

### 关键点
*   **应用场景与痛点**：针对智能体长文本推理场景（KV-Cache命中率常超95%），解决传统预填充-解码分离架构中预填充引擎（PE）网卡拥堵而解码引擎（DE）网卡闲置的I/O瓶颈。
*   **双路径架构**：打破传统的“存储至预填充”单路径，新增“存储→DE→PE”路径，将KV-Cache先读入DE缓冲池，再通过高速计算网络（RDMA）传给PE。
*   **核心组件**：系统由推理引擎（严格区分PE和DE）、流量管理器（负责拷贝与传输）和中央调度器（实时决策请求路径）三部分组成。
*   **流量管理优化**：采用以计算网卡（CNIC）为中心的流量管理，利用虚拟层技术将推理通信设为最高优先级并预留99%带宽，防止缓存搬运干扰模型计算。
*   **自适应调度**：调度器实时监控节点磁盘队列长度和Token数，优先将任务分配给I/O和计算压力较小的节点以避免拥塞。
*   **性能提升数据**：在660B规模生产级模型实测中，离线推理吞吐量提高1.87倍，在线服务吞吐量平均提升1.96倍。
*   **延迟优化**：在高负载下，首字延迟（TTFT）得到大幅优化，且Token间的生成速度（TPOT）几乎不受干扰，且未增加硬件成本。

## 正文

# DeepSeek联合北大清华发布DualPath框架：用闲置网卡加速智能体推理


DeepSeek与北京大学、清华大学在ArXiv上联合发布了一个全新的针对智能体的推理框架：**DualPath**。该框架的核心在于解决Agent长文本推理场景下的I/O瓶颈，通过优化从外部存储加载KV-Cache的速度，确保计算资源不被存储读取拖累。

在660B规模的生产级模型的实测中，DualPath表现惊人：离线推理吞吐量提高了1.87倍，在线服务吞吐量平均提升1.96倍。在高负载下，首字延迟（TTFT）大幅优化，而Token间的生成速度（TPOT）几乎不受任何干扰。

### 突破单路径瓶颈：双路径加载架构

DualPath是一个专门为智能体系统设计的推理框架，其核心洞见是：**KV-Cache的加载不必以预填充为中心**。

在当前的智能体应用中，对话轮数多且上下文长，KV-Cache命中率通常高达95%以上。这意味着每一轮对话都要搬运海量的“旧记忆”，推理性能的瓶颈已经从“计算”转移到了“搬运”上。正如业内专家反复强调的：计算是免费的，但数据移动是昂贵的。

在现有的预填充-解码分离（PD-disaggregated）架构中，所有的加载任务都拥挤在预填充引擎（PE）的存储网卡上，导致带宽瞬间饱和；与此同时，解码引擎（DE）的存储网卡却在闲置，造成了严重的资源错配。

针对这些问题，DualPath改变了传统的“存储至预填充（Storage-to-Prefill）”单路径加载模式，构建了创新的双路径模型：

*   **路径 A（传统）**：存储 → PE，缓存直接读入预填充引擎。
*   **路径 B（新增）**：存储 → DE → PE，缓存先读入解码引擎的缓冲池，再通过高速计算网络（RDMA）传输给预填充引擎。

在架构组成上，系统包含三个核心部分：
1.  **推理引擎**：每个引擎管理一块GPU，严格区分为预填充（PE）和解码（DE）。
2.  **流量管理器**：负责H2D/D2H拷贝、引擎间传输以及SNIC存储读写。
3.  **中央调度器**：担任“大脑”角色，实时决策每一条请求该走哪条路，从而实现全局带宽的最大化利用。

### 核心技术方案：存储至解码路径与流量调度

DualPath允许KV-Cache先加载至解码引擎（DE），再通过高带宽计算网络无损传输给预填充引擎（PE）。通过在两条路径间动态分配负载，系统将集群中原本闲置的解码侧存储网卡（SNIC）带宽彻底释放，构建起一个全局可调度的存储I/O资源池。

为了支持层级流式处理，DualPath在PE和DE上均分配了少量DRAM缓冲区，并设计了精细的数据流：
*   **PE读取路径**：命中Token的KV-Cache从存储读入PE缓冲区。在每层计算前，该层缓存传输至PE HBM，与计算过程重叠执行。计算完成后，全量KV-Cache传回DE缓冲区以形成完整上下文。
*   **DE读取路径**：KV-Cache直接进入DE缓冲区。在PE预填充期间，对应层的缓存跨节点传输至PE HBM（计算重叠）。计算结束后，PE仅需传回新生成的KV-Cache片段与DE原有缓存合并。
*   **解码与持久化**：DE缓冲区接收完整KV-Cache后启动解码，执行H2D拷贝并随后释放CPU内存。生成过程中，每累积满一个Block即触发异步持久化。

针对“绕路”加载可能带来的缓存搬运流量与模型计算通信冲突的问题，DualPath给出了两套优化方案：
1.  **以计算网卡（CNIC）为中心的流量管理**：强制所有流量通过配对的CNIC走GPUDirect RDMA路径。利用虚拟层（VL/TC）技术，将推理通信设为“最高优先级”并预留99%带宽，让缓存搬运只能在间隙中“蹭”带宽，确保互不干扰。
2.  **自适应请求调度器**：调度器实时监控每个节点的磁盘队列长度和Token数，优先将任务分配给I/O压力较小且计算负载较轻的节点，从根本上避免单侧网卡或单点计算资源的拥塞。

DualPath在DeepSeek-V3、Qwen等模型上进行了测试，成功证明了通过重新思考数据加载路径可以有效突破当前大模型推理的I/O墙。它在不增加硬件成本的前提下，大幅提升了智能体LLM推理系统的效率。

### 论文一作简介

该论文的第一作者吴永彤是北京大学的博士生，师从金鑫教授。他的研究方向聚焦于系统软件与大模型基础设施（LLM Infrastructure），尤其是推理系统的工程优化与规模化部署。他目前在DeepSeek系统组参与下一代模型的推理基础设施建设，负责大规模软件系统在多硬件平台上的性能优化。此前，他曾在腾讯、华盛顿大学、微软亚研院等机构实习。
