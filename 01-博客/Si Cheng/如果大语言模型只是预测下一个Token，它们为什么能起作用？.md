---
title: "如果大语言模型只是预测下一个Token，它们为什么能起作用？"
---

## 摘要

**1) 一句话总结**
大语言模型通过在海量包含人类推理的数据上最小化下一个Token的预测误差，结合注意力机制与庞大的参数规模，成功内化了语言中的逻辑与全局结构，从而展现出类似人类智能的输出能力。

**2) 核心要点**
*   **核心机制**：模型基于自回归建模（Autoregressive modeling），通过条件概率 $P(x_t \mid x_{<t})$ 仅根据先前的上下文来预测下一个Token。
*   **优化目标**：训练的核心目标是在数万亿个Token上最小化预测误差（交叉熵损失）。
*   **全局结构约束**：为了准确预测局部Token并降低损失，模型被迫学习并表示维持全局一致性的模式（如语法、微积分规则、代码逻辑），这些规则是被优化过程“筛选”出来的，而非人工编程。
*   **注意力机制（Attention）**：该机制允许每个Token直接关注其他所有Token以聚合上下文信息；其注意力权重通过反向传播和梯度下降在训练中不断更新，最终形成习得的路由系统。
*   **规模与涌现**：当模型达到数十亿参数和数万亿Token的规模时，会跨越“涌现阈值”，解锁维持长依赖关系和多步推理的能力。
*   **数据决定输出**：语言是人类思想的压缩记录，训练数据中包含了代码、数学证明和因果解释等结构化推理过程；模型通过逼近这些数据的统计分布，重现了结构化认知。
*   **工作模式转变**：在实际应用中，LLM作为重复性认知任务的“力量倍增器”，将工作模式从纯粹的“自动化”转向“增强”，使人类能专注于判断、验证和高层设计。

**3) 风险与局限性（基于原文）**
*   **缺乏真实认知**：模型本质上是统计推断系统，没有真正的“自我意识”、“信仰”或“意图”，其表现出的“理解”仅仅是外部观察到的行为特征。
*   **小模型的性能瓶颈**：在低于特定的模型容量（参数与规模）阈值时，网络无法有效表示复杂模式，容易在长逻辑链上失败并输出胡言乱语。
*   **无法完全替代人类**：模型不能取代一切工作，其生成的常规起草和框架内容仍需要人类进行最终的判断与验证。

## 正文

如果你像我一样了解过一些关于大语言模型（LLM）的基础知识，你可能一开始会感到有些失望：人工智能在本质上是一个庞大的模式匹配和统计推断系统。它并不是某种能够像人类一样思考的、极其酷炫或超级强大的系统，而只是一堆算法。统计推断意味着通过从数据中估计概率分布来学习模式，而不是通过手工编写明确的符号规则或逻辑。

从核心来看，像 ChatGPT 这样的模型是通过自回归建模（Autoregressive modeling）训练出来的，即仅根据序列中前面的 Token 来预测下一个 Token。

从形式上看，它将序列的概率分解为：
$P(x_1, x_2, \ldots, x_n) = \prod_t P(x_t \mid x_{<t})$

它并不像人类那样真正“知道”事实，它没有信仰，也没有意图。它只是根据给定的输入来预测下一个词。

但至少对我来说，这其中有一种非常奇妙的违和感。

当我在 ChatGPT 或其他 LLM 中输入一个问题时，得到的回复往往是连贯的、逻辑结构严密的，并且准确得令人惊讶。它能正确解释物理概念、编写可运行的代码、总结研究论文，简直就像在和一个恰好对你随机提出的问题非常了解的人交谈。如果这仅仅是基于统计的“下一个词预测”，它是如何产生出如此类似于“理解”的内容的？或者说，如果我只是在键盘上随机输入问题，然后疯狂点击输入法给出的第一个候选词，为什么我的键盘无法生成同样的输出？

最让我感到好奇的是：一方面，我们有一个相对简单的训练目标——在数以万亿计的 Token 上最小化预测误差（即交叉熵损失，它衡量了预测概率分布与真实分布之间的差异，交叉熵越低意味着模型赋予正确的下一个 Token 的概率越高）。但另一方面，我们却观察到了类似于推理、抽象甚至创造力的行为。

这不禁让我思考：模式匹配是如何转变为让人感觉具备智能的东西的？

### 局部预测与全局结构

为了回答这个问题，我们需要更仔细地审视在这种语境下“模式匹配”到底意味着什么，因为预测下一个 Token 并不等同于随机选择一个看起来相似的词。

从形式上看，模型试图逼近条件概率 $P(x_t \mid x_{<t})$，这代表了在给定所有先前 Token 的情况下，下一个 Token 出现的概率。这也是 LLM 训练旨在逼近的核心目标：

**基于目前为止所说的一切，下一个 Token 的概率是多少？**

听起来很简单，一次只预测一个词，就像你普通的手机键盘一样。然而，如果你了解 LLM 的上下文机制，就会知道下一个 Token 的概率取决于**整个**先前的上下文。

因此，如果上下文是一个数学证明，下一个 Token 必须遵循逻辑一致性；如果上下文是一个 Python 函数，下一个 Token 必须遵守语法规则；如果上下文是一个物理学解释，下一句话必须保持因果关系。

如果我们把预测看作是一个局部过程，那它将是一个简单的任务。但“正确性”是全局的，在数万亿个样本中最小化预测误差，迫使模型必须去表示那些能够维持全局一致性的模式。

考虑一个简单的例子，如果模型看到的输入是：`Let f(x) = x^2 + 3x. Then f'(x) =`

正确的延续并不是由词频决定的，而是受微积分规则约束的。为了预测正确的下一个 Token，模型必须逼近那些表现得像导数规则一样的模式。在大量的样本中，如果未能捕捉到这些规律，预测误差就会增加。

同样的情况也适用于代码。如果模型生成：
```python
def add(a, b):
    return
```
下一个 Token 是受函数语义约束的，而不是受表面相似性约束的。换句话说，准确的预测需要内部表示能够捕捉到与特定领域规则相一致的规律。

这就是关键所在：训练目标并没有明确告诉模型去学习逻辑、语法或物理，但未能表示这些结构会增加损失（Loss）。损失函数是训练期间需要最小化的数值目标，它量化了模型的预测与真实的下一个 Token 之间的差距。因此，结构或规则并不是被“编程”进模型中的，而是被优化过程“筛选”出来的。正如你所见，我们所谓的“规则”，可能仅仅是数据中高度稳定的统计规律，而这正是 LLM 或统计模型所擅长的。

### 注意力机制与上下文建模

回想一下，模型试图逼近的是 $P(x_t \mid x_{<t})$。

这意味着，在时间步 $t$，当前 Token 的表示必须编码来自先前 Token 的所有相关信息。从工程角度来看，这是一个上下文聚合问题，因为模型必须回答：

**“之前序列中的哪些部分对预测下一个 Token 是重要的？”**

简单的前馈神经网络无法很好地完成这项工作，因为一旦输入被转化为嵌入向量（Embedding，即将词作为连续的高维向量而非符号来处理），它就会独立地处理这些输入。

即便如此，像 RNN 这样的早期序列模型也面临困境，因为信息必须按顺序流经一个固定大小的隐藏状态。长距离的依赖关系会被压缩并最终被稀释。然而，注意力机制（Attention）以不同的方式解决了这个问题：它不再将所有过去的信息压缩成一个单一的向量，而是允许每个 Token 直接“看向”其他每一个 Token。注意力权重决定了一个 Token 对另一个 Token 的影响程度，它们在训练期间通过梯度更新被学习出来。

### 损失函数作为结构的筛选器

但注意力本身只是一种机制，更重要的问题是：

**这种机制是如何与预测目标对齐的？**

在训练期间，模型会根据它预测下一个 Token 的表现来计算损失。来自该损失的梯度（即损失相对于模型参数的导数，指示参数应如何改变以减少误差）会通过反向传播算法（Backpropagation）向后流经整个网络，包括注意力权重。

因此，如果关注某些特定的 Token 有助于减少预测误差，相应的注意力权重就会被加强；如果这些连接增加了误差，它们就会被削弱。经过数十亿次的更新，注意力机制变成了一个直接由优化过程塑造的、习得的路由系统。

这个参数更新过程是通过梯度下降来优化的，损失信号会传播到所有层，更新嵌入向量、注意力投影和输出层。

换句话说：
*   损失塑造了注意力。
*   注意力塑造了表示。
*   表示塑造了预测。

而预测，是模型唯一被明确训练去执行的任务。

### 为什么大语言模型的输出看起来具备智能？

#### 规模带来的涌现能力

到目前为止，我们描述的一切听起来都不像魔法：
*   我们有一个条件概率目标。
*   我们有一个用于上下文建模的机制。
*   我们有基于梯度的优化来塑造内部表示。

这一切听起来都很合理，但这种显而易见的“智能”到底从何而来？为什么小模型往往只会输出胡言乱语？

缺失的拼图是**规模**。

大型 Transformer 是容量极高的函数逼近器。拥有数十亿个参数和数万亿个 Token，它们不仅仅是在学习表面的词汇相关性，而是在逼近一个高度复杂的、关于语言的条件分布。

而规模化的语言数据并不是随机的文本，它包含了：
*   结构化的推理
*   逐步的推导
*   具有功能依赖关系的代码
*   数学证明
*   因果解释
*   议论文
*   问题解决的轨迹

当模型在这样的数据上最小化下一个 Token 的预测误差时，它被迫内化这些模式的统计结构。如果数据中存在推理轨迹，那么正确预测下一个 Token 往往需要对生成这些轨迹的推理过程进行建模。

换句话说，模型并没有被明确训练去学习推理算法，但它被训练去预测由推理过程生成的输出。如果这些输出在统计上是可学习的，那么模型就会去逼近它们。

因此，在低于特定的模型容量时，网络无法很好地表示这些模式。它可能会捕捉到局部语法，但在长逻辑链上会失败。然而，随着规模的增加，模型可以表示更复杂的函数，突然之间：
*   注意力层可以维持更长的依赖关系。
*   表示变得更具表现力。
*   多个抽象层次可以共存。
*   复杂的 Token 交互变得可学习。

在某个阈值（涌现阈值）之上，模型变得能够对类似于多步推理的模式进行建模，这就是我们所说的“涌现”。涌现并不意味着魔法；它通常指的是只有在扩大模型规模、数据量和优化预算后才会显现的能力飞跃。规模放大了能力。

#### 语言作为压缩的推理过程

另一种看待这个问题的方式是认识到：语言本身就是人类思想的压缩记录。当人类编写解释、代码或证明时，我们将内部的推理外化为 Token 序列。在海量语料库上训练模型预测下一个 Token，等同于学习这些推理轨迹在统计上是如何展开的。

所以从根本上说，模型并没有模拟一个大脑；它只是在逼近推理输出的分布。但如果这个分布编码了结构化的认知，那么逼近这个分布不可避免地会产生类似于结构化认知的输出。

#### 为什么这感觉像是一种“理解”？

最后一个问题有点偏心理学层面：

**为什么这会让人感觉像是一种“理解”？**

因为从外部来看，我们是通过输出行为来判断智能的。

因此，如果一个系统能够：
*   产生连贯的论点
*   编写语法正确且功能完备的代码
*   遵循逻辑约束
*   在长上下文中保持一致性

我们就会很自然地将智能归因于它。但所有这些属性在文本中都是可观察到的，如果这些属性可以从数据中进行统计学习，那么一个经过训练以最小化预测误差的、足够大的模型就能重现它们。

在这种操作性意义上（这是一种行为定义：如果输出始终满足推理约束，从外部评估者的角度来看，系统就显得很智能），“理解”变得与对结构化数据进行高质量的预测建模无法区分。

模型根本没有我们所说的“自我意识”或“信仰”，但它已经学会了产生与人类推理模式相一致的输出，而这正是我们所感知到的智能。

### 总结

回到最初的问题：

**如果大语言模型只是预测下一个 Token，它们为什么能起作用？**

我认为现在的等式已经非常清晰了：
*   预测迫使结构产生。
*   优化筛选出一致性。
*   注意力路由信息。
*   规模解锁容量。
*   数据包含推理。

因此，归根结底，更务实的看法不是“大语言模型将取代一切”，而是“大语言模型将重新分配工作”。在实践中，许多团队看到了向“增强”而非纯粹“自动化”的转变：常规的起草、总结和搭建框架首先被自动化，而人类的精力则转向判断、验证和高层次的设计。它们作为重复性认知任务的“力量倍增器”时最为强大，这给了我们更多的空间去专注于创意方向、问题框架和决策制定。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
