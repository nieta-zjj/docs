---
title: "别再问模型是否具有可解释性了"
发布日期: 2026-02-27
作者: "Towards Data Science"
来源: "Towards Data Science"
原文链接: "https://towardsdatascience.com/stop-asking-if-a-model-is-interpretable/"
译注: "根据原文翻译整理"
---

## 摘要

**1) 一句话总结**
AI模型的可解释性并非一种非黑即白的固有属性，而是一组用于回答特定问题的方法，主要服务于诊断故障、验证学习和提取知识三大核心科学功能。

**2) 关键要点**
*   **转变认知框架**：不应将可解释性视为模型“有或无”的属性或单一的复选框，而应关注“我们需要解释什么”。
*   **三大核心功能**：在实践中，可解释性服务于三个截然不同的目标：诊断故障、验证学习和提取知识。
*   **功能一：作为诊断（开发阶段）**：类似于软件调试工具，用于识别聚合指标无法揭示的隐藏故障模式（例如：通过可视化确认MNIST分类模型是关注数字笔画还是无关背景）。
*   **功能二：作为验证（成功模型）**：用于确认表现良好的模型是否“因为正确的原因而成功”，检查其内部表示是否符合领域预期（例如：验证ImageNet模型是否学习到了从边缘到形状的层次特征，而非依赖浅层视觉线索）。
*   **功能三：作为知识（发现工具）**：在仅靠预测不够的领域（如科学应用），可解释性可揭示新的假设或未注意到的关系（例如：在CT扫描中发现临床医生以前未关注到的与诊断相关的微妙结构）。
*   **上下文依赖**：可解释性的要求始终取决于具体的应用场景（开发、研究或部署）。
*   **人机接口**：可解释性最好被理解为人类与模型之间的接口，它使不透明的预测转化为可进行科学分析的对象。

**3) 风险/差距**
*   **虚假关联风险**：模型可能依赖偶然的或虚假的关联（如背景线索）来实现高准确率，这在科学上具有误导性，意味着模型学到了错误的概念。
*   **指标掩盖故障**：仅依赖准确率等聚合性能指标，可能会掩盖模型在概念上的失败或隐藏的故障模式。
*   **信任缺失风险**：在医疗成像等关键领域，如果模型解释出的决策依据（如高亮区域）不符合专业推理，无论其预测准确率多高，该系统都无法被信任。

## 正文

**作者**：Manuel Franco de la Peña
**核心观点**：别再问模型是否具有可解释性了，开始问解释应该回答什么问题。

大多数关于 AI 可解释性的讨论都始于一个错误的问题。研究人员、从业者甚至监管机构经常问一个模型*是否具有可解释性*。但这种框架假设可解释性是模型要么拥有要么缺乏的属性。事实并非如此。

模型在抽象意义上并不存在可解释或不可解释之说。这里我们讨论的不是本质上透明的模型（如线性回归或决策树，它们的推理可以直接检查），相反，我们关注的是决策过程无法立即访问的复杂模型。

因此，可解释性不是一个复选框、一个可视化或一个特定的算法。它更好地被理解为一组允许人类分析模型以回答特定问题的方法。改变问题，解释的有用性也会随之改变。那么，真正的问题不在于模型是否具有可解释性，而在于我们需要解释什么。

一旦我们以这种方式看待可解释性，一个更清晰的结构就会出现。在实践中，解释始终服务于三个截然不同的科学功能：诊断故障、验证学习和提取知识。这些角色在概念上是不同的，即使它们依赖于类似的技术。理解这种区别有助于阐明何时需要可解释性以及我们实际需要什么样的解释。

### 作为诊断的可解释性

可解释性的第一个作用出现在模型开发期间，此时模型仍是实验对象。在这个阶段，它们是不稳定的、不完美的，并且经常以聚合指标无法揭示的方式出错。准确性告诉我们模型是否成功，但不能告诉我们它为什么失败。两个模型可以实现相同的性能，同时依赖于完全不同的决策规则。一个可能正在学习真实的结构；另一个可能正在利用偶然的关联。

可解释性方法允许我们查看模型的内部决策过程并识别这些隐藏的故障模式。从这个意义上说，它们发挥的作用类似于软件工程中的调试工具。没有它们，改进模型在很大程度上变成了猜测。有了它们，我们可以提出关于模型实际在做什么的可测试假设。

一个简单的例子来自手写数字分类。MNIST 数据集故意做得很简单，这使得它非常适合检查模型的推理是否符合我们的预期。当我们可视化哪些像素影响了预测时，我们可以立即看到网络是关注数字笔画还是不相关的背景区域。这种差异告诉我们模型是学习到了有意义的信号还是走捷径。在这种诊断角色中，解释不是为最终用户或利益相关者准备的，它们是开发人员试图理解模型行为的工具。

### 作为验证的可解释性

一旦模型表现良好，问题就变了。我们不再主要关心它为什么失败。相反，我们想知道它是否因为正确的原因而成功。

这种区别很微妙但至关重要。如果一个系统依赖于虚假的关联，它可能会实现高准确率，但在科学上仍然具有误导性。例如，一个训练用于检测动物的分类器可能看起来工作得非常完美，而实际上它依赖于背景线索而不是动物本身。从预测的角度来看，这样的模型看起来是成功的；但从科学的角度来看，它学到了错误的概念。

可解释性允许我们检查内部表示并验证它们是否符合领域预期。在深度神经网络中，中间层编码学习到的特征，分析这些表示可以揭示系统是发现了有意义的结构还是仅仅记住了表面模式。

这在 ImageNet 等大规模自然图像数据集中变得尤为重要，其中场景在视角、背景和对象外观方面包含大量变化。因为 ImageNet 图像包含杂乱的场景、多样的上下文和高类内变异性，成功的模型必须学习层次表示，而不是依赖于浅层的视觉线索。当我们可视化内部过滤器或激活图时，我们可以检查早期层是否检测到边缘，中间层是否捕获纹理，以及更深层是否响应形状。这种结构的存在表明网络已经学到了关于数据的一些有意义的东西。它的缺失表明性能指标可能掩盖了概念上的失败。

在这第二个角色中，可解释性不是调试损坏的模型，而是验证成功的模型。

### 作为知识的可解释性

当模型应用于仅靠预测是不够的领域时，第三个作用就出现了。在这些背景下，机器学习系统不仅用于产生输出，还用于产生见解。在这里，可解释性成为发现的工具。

现代模型可以检测跨数据集的统计规律，这些数据集远大于任何人类可以手动分析的数据集。当我们能够检查它们的推理时，它们可能会揭示暗示新假设或以前未注意到的关系的模式。在科学应用中，这种能力通常比预测准确性本身更有价值。

医学成像提供了一个清晰的例子。考虑一个训练用于从 CT 扫描中检测肺癌的神经网络。如果这样的模型预测恶性肿瘤，临床医生需要了解哪些区域影响了该决定。如果突出显示的区域对应于肿瘤边界，则解释符合医学推理。如果不是，无论其准确性如何，都不能信任该预测。

但还有第三种可能性：解释可能会揭示临床医生以前未认为在诊断上相关的微妙结构。在这些情况下，可解释性不仅证明了预测的合理性，它还为知识做出了贡献。在这里，解释不仅仅是理解模型的工具，它们是扩展人类理解的工具。

### 一个概念，三个功能

这些例子说明的是，可解释性不是一个单一的目标，而是一个多功能的框架。相同的技术可以帮助调试模型、验证其推理或提取见解，具体取决于所问的问题。关于可解释性的混淆通常是因为讨论未能区分这些目标。

更有用的问题不是模型是否具有可解释性，而是它对于我们关心的任务是否足够可解释。该要求始终取决于上下文：开发、研究或部署。

以这种方式看待，可解释性最好被理解为人类和模型之间的接口，而不是对机器学习的约束。正是它允许我们进行诊断、验证和学习。没有它，预测仍然是不透明的输出。有了它，它们就成了科学分析的对象。

因此，与其问模型是否具有可解释性，我们应该问一个更精确的问题：

**我们到底希望解释什么？**

一旦这个问题明确了，可解释性就不再是一个模糊的要求，而成为一种科学工具。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
