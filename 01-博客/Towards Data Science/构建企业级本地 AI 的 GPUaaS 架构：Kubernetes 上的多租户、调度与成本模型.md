# 构建企业级本地 AI 的 GPUaaS 架构：Kubernetes 上的多租户、调度与成本模型

## 文档信息
- 来源：https://towardsdatascience.com/architecting-gpuaas-for-enterprise-ai-on-prem/
- 发布日期：2026-02-21

## 摘要
### 1) 一句话总结
基于 Cisco UCS 服务器和单节点 OpenShift，本文详细介绍了一种企业级本地 GPUaaS 架构的设计与实现，涵盖了多租户隔离、基于 MIG 与时间切片的 GPU 调度机制以及量化的 AI 算力成本模型。

### 2) 关键要点
*   **硬件与底层平台**：基于 Cisco UCS C845A 服务器（双 NVIDIA RTX PRO 6000 Blackwell GPU、3.1TB NVMe、754GB 内存），部署为单节点 OpenShift (SNO) 集群。
*   **GPU 分区与调度**：采用混合 MIG 策略，GPU 0 划分为 4 个 `1g.24gb` 切片，GPU 1 保持完整（约 96GB）；结合时间切片技术（完整 GPU 设 4 个副本，MIG 切片设 2 个副本）支持高并发。
*   **三层架构设计**：系统分为调度平面（FastAPI/React 预订界面）、控制平面（Python 协调器，30秒循环同步状态）和运行时平面（隔离的 K8s 命名空间）。
*   **单一事实来源**：PostgreSQL 数据库作为系统状态的唯一事实来源，FastAPI 仅与数据库交互，仅由协调器（Reconciler）负责与 Kubernetes API 通信以执行幂等操作。
*   **多租户与安全隔离**：每个会话分配独立的命名空间、600GB 存储和预装 AI 栈的 VS Code 工作区；通过 ResourceQuota 强制执行硬性资源上限，防止 OOM 或存储耗尽。
*   **两种 GPU 分配模式**：支持“交互式机器学习”（工作区直接挂载 GPU）和“推理容器”（工作区轻量化，GPU 资源全部分配给用户部署的容器）两种模式。
*   **节点级模型缓存**：部署缓存守护进程预先加载大型模型工件（如 20GB 的 vLLM 镜像），消除冷启动延迟。
*   **量化成本模型（代币经济学）**：提出“每百万 Token 成本”框架，指出硬件利用率是降低单位成本的核心杠杆；对于持续稳定的多智能体推理流量，本地部署成本低于云端。

### 3) 风险与不足
*   **时间切片的性能损耗**：时间切片会导致工作负载共享 VRAM 和计算带宽，在吞吐量上存在妥协；对于生产环境中对延迟敏感的推理任务，必须使用 1:1 的专用切片或完整的 GPU。
*   **资源类型分配风险**：在混合 MIG 环境中，如果允许会话消耗错误的 GPU 资源类型，将破坏整个预订系统的容量计算（系统通过 ResourceQuota 硬性拒绝来规避此风险）。
*   **硬件与断电故障**：系统运行面临节点电源故障等导致集群中途宕机的风险（架构通过数据库保存期望状态及协调器的自我修复机制来应对）。
*   **本地部署的成本劣势场景**：虽然本地部署在持续的企业级流量下具有成本优势，但对于测试、演示和概念验证（POC）场景，云端方案的成本更低。

## 正文
AI 技术正在快速演进，软件工程师不再需要死记硬背语法。然而，像架构师一样思考，并理解如何让系统在规模化下安全运行的技术，正变得越来越有价值。

作为思科（Cisco）的 AI 解决方案工程师，我在这个岗位上已经工作了一年。我每天都在与医疗保健、金融服务、制造业和律师事务所等不同垂直领域的客户打交道，他们都在试图解答大致相同的一系列问题：

*   我们的 AI 战略是什么？
*   哪些用例真正适合我们的数据？
*   选择云端、本地还是混合架构？
*   成本是多少——不仅是现在，而是在规模化之后？
*   我们如何保障其安全性？

当你试图将 AI 从概念验证（POC）推向实际运营时，这些都是会立即显现的现实约束。

最近，我们在实验室中增加了一台 Cisco UCS C845A 服务器。它配备了 2 张 NVIDIA RTX PRO 6000 Blackwell GPU、3.1TB NVMe 存储、约 127 个可分配的 CPU 核心以及 754GB 内存。我决定在此基础上构建一个共享的内部平台——为团队提供一个一致的、自助式的环境，以便运行实验、验证想法并积累 GPU 的实操经验。

我将该平台部署为单节点 OpenShift（SNO）集群，并在其上构建了多租户的 GPUaaS（GPU 即服务）体验。用户通过日历 UI 预订计算能力，系统会配置一个隔离的机器学习环境，其中预装了 PyTorch/CUDA、JupyterLab、VS Code 等。在该环境中，用户可以运行按需推理、迭代模型训练和微调，并构建生产级别的微服务原型。

本文将详细介绍该架构——调度决策是如何制定的、租户是如何隔离的，以及平台是如何进行自我管理的。这个实验室平台所涉及的决策，正是任何认真对待生产环境 AI 的组织都会面临的决策。

这是规模化企业 AI 的基础。多智能体架构、自助式实验、安全的多租户、成本可预测的 GPU 计算，这一切都始于构建正确的平台层。

### 初始设置

在平台建立之前，我们面对的只是一台裸金属服务器和一个空白屏幕。

#### 节点引导
该节点出厂时没有操作系统。开机后会进入 UEFI shell。对于 OpenShift，安装通常从 Red Hat Hybrid Cloud Console 通过 Assisted Installer（辅助安装程序）开始。Assisted Installer 通过引导式设置流程处理集群配置，完成后会生成一个发现 ISO——这是一个为你的环境预配置的可启动 RHEL CoreOS 镜像。通过 Cisco IMC 将该 ISO 作为虚拟媒体映射到服务器，设置启动顺序并开机。节点会向控制台发送信号，随后即可启动安装过程。节点将 RHCOS 写入 NVMe 并进行引导。几个小时内，你就能拥有一个运行中的集群。

这种工作流假设有互联网连接，在安装期间从 Red Hat 的注册表拉取镜像。但这并不总是可行的。我合作的许多客户都在隔离（Air-gapped）环境中运行，没有任何东西连接到公共互联网。那里的流程有所不同：在本地生成 ignition 配置，提前下载 OpenShift 发布镜像和 operator bundle，将所有内容镜像到本地的 Quay 注册表中，并将安装指向该注册表。这两条路径最终殊途同归。辅助安装要简单得多，而隔离路径则是受监管行业生产环境的真实写照。

#### 使用 NVIDIA GPU Operator 配置 GPU
安装 GPU Operator（使用辅助安装程序会自动完成）后，我通过 `nvidia-gpu-operator` 命名空间中的两个 ConfigMap，配置了如何将两张 RTX PRO 6000 Blackwell GPU 呈现给工作负载。

第一个是 `custom-mig-config`，定义了物理分区。这里采用的是混合策略，即 GPU 0 被划分为四个 `1g.24gb` 的 MIG 切片（每个切片约 24GB 专用内存），而 GPU 1 保持完整，供需要全部约 96GB 内存的工作负载使用。MIG 分区是真正的硬件隔离。每个切片都有专用的内存、计算单元和 L2 缓存。工作负载会将 MIG 实例视为独立的物理设备。

第二个是 `device-plugin-config`，配置了时间切片（Time-slicing），这允许多个 Pod 通过快速的上下文切换共享同一个 GPU 或 MIG 切片。我为完整的 GPU 设置了 4 个副本，为每个 MIG 切片设置了 2 个副本。这使得在单个会话中可以并排运行多个推理容器。

#### 基础存储
3.1TB 的 NVMe 由 LVM Storage Operator（`lvms-vg1` StorageClass）管理。作为初始配置过程的一部分，我创建了两个 PVC——一个用于支持 PostgreSQL 的卷，另一个用于 OpenShift 内部镜像注册表的持久化存储。

随着操作系统的安装、网络先决条件的满足（DNS、IP 分配、所有必需的 A 记录，本文不作赘述）、GPU 的分区以及存储的配置完成，集群已经为应用层做好了准备。

### 系统架构

这引出了我们的核心主题：系统架构。该平台分为三个平面——调度平面、控制平面和运行时平面，并以 PostgreSQL 数据库作为单一事实来源。

在平台管理命名空间中，有四个始终运行的部署：

*   **Portal app（门户应用）**：运行 React UI 和 FastAPI 后端的单一容器。
*   **Reconciler（协调器/控制器）**：持续使集群状态与数据库保持一致的控制循环。
*   **PostgreSQL**：用于用户、预订、令牌和审计历史的持久化状态。
*   **Cache daemon（缓存守护进程）**：一个节点本地服务，用于预先加载大型模型工件/推理引擎，以便用户可以快速启动（通过企业代理拉取 20GB 的 vLLM 镜像可能需要几个小时）。

关于开发生命周期需要简要说明一下，因为在 Kubernetes 系统中发布软件很容易变得复杂。我在本地编写和测试代码，但镜像是使用 OpenShift 构建工件（BuildConfigs）在集群内构建的，并推送到内部注册表。部署本身只需指向这些镜像。

第一次引入组件时，我应用清单文件来创建 Deployment/Service/RBAC。之后，大多数更改只需在集群内构建新镜像，然后触发重启，Deployment 就会拉取更新后的镜像并向前滚动：

这就是循环：`提交代码 → 集群内构建 → 内部注册表 → 重启/推出`。

#### 调度平面
这是面向用户的入口点。用户看到资源池——GPU、CPU、内存，他们选择一个时间窗口，选择 GPU 分配模式（后文会详细说明），然后提交预订。

GPU 是昂贵的硬件，无论是否使用，每小时都有实际成本。预订系统将日历时间和物理容量视为一个组合约束。这就像你预订会议室一样，只不过这个“房间”有 96GB 的显存，而且每小时的成本要高得多。

在底层，系统使用咨询锁（advisory locks）查询重叠的预订与资源池容量，以防止重复预订。本质上，它只是将已预订的容量相加，并从总容量中减去。每个预订都会经历一个生命周期：`APPROVED`（已批准） → `ACTIVE`（活跃） → `COMPLETED`（已完成），其中 `CANCELED`（已取消）和 `FAILED`（失败）为终止状态。

FastAPI 服务器本身被设计得很轻量。它验证输入，持久化预订记录，然后返回。它从不与 Kubernetes API 通信。

#### 控制平面
平台的核心是控制器（Reconciler）。它基于 Python 编写，以 30 秒为周期在连续循环中运行。在时间安排上，你可以把它想象成一个 cron 任务，但在架构上，它是一个 Kubernetes 风格的控制器，负责驱动系统达到期望状态。

数据库保存着期望状态（带有时间窗口和资源需求的预订）。协调器读取该状态，将其与 Kubernetes 集群中实际存在的状态进行比较，并使两者趋于一致。这里没有并发的 API 调用竞相改变集群状态；只有一个确定性的循环，进行达到期望状态所需的最小更改集。如果协调器崩溃，它会重启并准确地从中断的地方继续，因为事实来源（期望状态）在数据库中保持完好。

每个协调周期按顺序评估四个关注点：

1.  停止过期或取消的会话，并删除命名空间（这会级联清理其中的所有资源）。
2.  修复失败的会话，并删除由部分完成的配置留下的孤立资源。
3.  当预订窗口到来时启动符合条件的会话——配置环境并将会区移交给用户。
4.  通过使旧令牌过期并强制执行审计日志保留策略来维护数据库。

启动会话是一个多步配置序列，每一步都是幂等的，这意味着如果中途被中断，它可以安全地重新运行。

协调器是唯一与 Kubernetes API 通信的组件。

垃圾回收也内置在同一个循环中。以较慢的节奏（约 5 分钟），协调器会扫描跨命名空间的孤立资源，例如陈旧的 RBAC 绑定、残留的 OpenShift 安全上下文条目、卡在终止状态的命名空间，或者存在于集群中但在数据库中没有匹配记录的命名空间。

整个设计假设失败是常态。例如，我们的节点曾发生过电源故障，导致集群在会话中途宕机，当它恢复时，协调器恢复了其循环，检测到状态差异，并在无需人工干预的情况下进行了自我修复。

#### 运行时平面
当预订窗口开始时，用户打开浏览器，进入一个完整的 VS Code 工作区（code-server），其中预装了整个 AI/ML 技术栈，并在其会话命名空间内拥有 kubectl 访问权限。

流行的推理引擎（如 vLLM、Ollama、TGI 和 Triton）已经缓存在节点上，因此部署模型服务器只需一行命令，几秒钟即可启动。会话分配了 600GB 由 NVMe 支持的持久化存储，包括用于笔记本和脚本的 20GB 主目录，以及 300GB 的模型缓存。

每个会话都是一个完全隔离的 Kubernetes 命名空间，拥有自己的爆炸半径边界、专用资源，并且对任何其他租户的环境零可见性。协调器配置了命名空间范围的 RBAC，授予该边界内的完全管理员权限，使用户能够创建和删除 Pod、Deployment、Service、Route、Secret——任何工作负载所需的内容。但没有集群级别的访问权限。用户可以读取自己的 ResourceQuota 以查看剩余预算，但不能修改它。

ResourceQuota 对所有内容强制执行硬性上限。失控的训练任务无法导致节点 OOM（内存溢出）。恶意容器无法填满 NVMe。LimitRange 会自动将合理的默认值注入每个容器，因此用户可以直接运行 `kubectl run` 而无需指定资源请求。命名空间中注入了一个代理 ConfigMap，因此用户部署的容器无需手动配置即可获得企业网络出口权限。

用户部署他们想要的内容——推理服务器、数据库、自定义服务，而平台负责提供护栏。

当预订窗口结束时，协调器会删除命名空间及其内部的所有内容。

### GPU 调度

现在到了最有趣的部分——GPU 调度以及在多租户环境中实际运行硬件加速的工作负载。

#### MIG 与时间切片
我们在初始设置中介绍了 MIG 配置，但从调度的角度来看，它值得再次探讨。GPU 0 被划分为四个 `1g.24gb` MIG 切片——每个切片拥有约 24GB 的专用内存，足以满足大多数 7B–14B 参数模型的需求。GPU 1 保持完整，供需要全部约 96GB VRAM 的工作负载使用，例如模型训练、70B+ 模型的全精度推理，或者任何根本无法放入单个切片的工作负载。

预订系统将这些作为不同的资源类型进行跟踪。用户预订 `nvidia.com/gpu`（完整）或 `nvidia.com/mig-1g.24gb`（最多四个切片）。每个会话的 ResourceQuota 会硬性拒绝相反的类型。如果你预订了 MIG 切片，你在物理上就无法请求完整的 GPU，即使有一张 GPU 处于空闲状态。在混合 MIG 环境中，如果允许会话意外消耗错误的资源类型，将会破坏日历上所有其他预订的容量计算。

时间切片允许多个 Pod 通过快速上下文切换共享同一个物理 GPU 或 MIG 切片。NVIDIA 设备插件为每个物理设备通告 N 个“虚拟”GPU。

在我们的配置中，1 个完整的 GPU 显示为 4 个可调度资源。每个 MIG 切片显示为 2 个。

这意味着用户预订了一个物理 GPU，就可以在其会话中运行最多四个并发的 GPU 加速容器——一个提供 gpt-oss 服务的 vLLM 实例、一个运行 Mistral 的 Ollama 实例、一个运行重排序器（reranker）的 TGI 服务器，以及一个跨这三者进行编排的自定义服务。

#### 两种分配模式
在预订时，用户选择其 GPU 预算最初如何在工作区和用户部署的容器之间分配。

*   **交互式机器学习（Interactive ML）**：工作区 Pod 直接附加一个 GPU（或 MIG 切片）。用户打开 Jupyter，导入 PyTorch，并立即获得用于训练、微调或调试的 CUDA 访问权限。额外的 GPU Pod 仍然可以通过时间切片生成，但工作区正在消耗其中一个虚拟插槽。
*   **推理容器（Inference Containers）**：工作区是轻量级的，没有附加 GPU。所有时间切片容量都可用于用户部署的容器。对于完整的 GPU 预订，这意味着有四个完整的插槽用于推理工作负载。

时间切片在吞吐量方面存在真实的权衡，工作负载共享 VRAM 和计算带宽。对于开发、测试和验证多服务架构（这正是该平台的用途），这是正确的权衡。对于生产环境中对延迟敏感的推理（每一毫秒的 p99 延迟都很重要），你应该使用 1:1 的专用切片或完整的 GPU。

### GPU “代币经济学”（Tokenomics）

引言中的第一个问题是：成本是多少——不仅是现在，而是在规模化之后？要回答这个问题，你必须从生产环境中工作负载的实际情况开始。

#### 真实部署场景
当我与客户合作设计他们的推理架构时，没有人会在单个端点后面运行单个模型。不断出现的模式是根据任务调整大小的模型舰队。你有一个 7B 参数模型处理简单的分类和提取，在 MIG 切片上运行得很轻松。一个 14B 模型进行摘要和通用聊天。一个 70B 模型用于复杂的推理和多步任务，也许还有一个 400B 模型用于质量不容妥协的最困难问题。请求会根据复杂性、延迟要求或成本约束路由到适当的模型。你不会为了一个 7B 模型就能处理的任务去支付 70B 级别的计算成本。

在多智能体（Multi-agent）系统中，这变得更加有趣。智能体订阅消息总线并保持空闲直到被调用——这是一种发布-订阅模式，在调用时将上下文共享给智能体，并且 Pod 已经是预热状态。没有冷启动惩罚，因为模型已加载且容器正在运行。编排器智能体评估入站请求，将其路由到专家智能体（检索、代码生成、摘要、验证），收集结果并合成响应。四五个模型协作处理单个用户请求，每个模型在同一个命名空间内的独立容器中运行，通过内部 Kubernetes 网络进行通信。

网络策略增加了另一个维度。并非每个智能体都应该访问每个工具。你的检索智能体可以与向量数据库通信。你的代码执行智能体可以访问沙盒运行时。但摘要智能体无权接触这两者，它只从编排器接收上下文并返回文本。网络策略在集群级别强制执行这些边界，因此工具访问由基础设施控制，而不是应用程序逻辑。

这就是该平台设计所针对的工作负载配置文件。MIG 切片让你能够为每个模型分配合适大小的 GPU，一个 7B 模型不需要 96GB 的 VRAM。时间切片允许多个智能体共享同一个物理设备。命名空间隔离使租户保持分离，而会话内的智能体可以自由通信。该架构直接支持这些模式。

#### 量化成本
为了从架构转向商业案例，我开发了一个代币经济学框架，将基础设施成本简化为一个单一的可比单位：**每百万 Token 的成本**。每个 Token 都承担了其分摊的硬件资本（包括工作负载组合和冗余）、维护、电力和冷却成本。分子是你的年度总成本。分母是你实际处理的 Token 数量，这完全取决于利用率。

利用率是影响单 Token 成本的最强大杠杆。它不会减少你的支出，硬件和电费是固定的。它的作用是将这些固定成本分摊到更多处理过的 Token 上。以 80% 利用率运行的平台，其生产 Token 的单位成本几乎是以 40% 运行的平台的一半。相同的基础设施，截然不同的经济效益。这就是为什么预订系统、MIG 分区和时间切片在用户体验之外如此重要——它们的存在是为了让昂贵的 GPU 在尽可能多的可用时间内处理 Token。

因为该框架是代数性质的，你也可以反向求解。给定已知的 Token 需求和预算，求解所需的基础设施，并立即看出你是过度配置（在空闲 GPU 上烧钱）、配置不足（请求排队并降低延迟）还是配置得当。

对于云服务的比较，提供商已经将他们的利用率、冗余和开销计入了每 Token 的 API 定价中。问题变成了：在什么利用率下，你的本地单位成本会降至该费率以下？对于持续的企业 GPU 需求，即这些多智能体架构产生的稳定状态推理流量，本地部署胜出。

在多智能体环境中，云端 Token 成本呈抛物线式增长。

然而，对于测试、演示和 POC，云端更便宜。

工程团队通常需要用清晰、站得住脚的数字向财务部门证明支出的合理性。代币经济学框架弥合了这一差距。

### 结论

在本文开头，我列出了我经常从客户那里听到的问题——AI 战略、用例、云端与本地、成本、安全。它们最终都需要同一个东西：一个能够调度 GPU 资源、隔离租户，并为团队提供从实验到生产的自助路径而无需等待基础设施的平台层。

这就是本文所探讨的内容。它不是一个产品，也不是一个托管服务，而是一个基于 Kubernetes、PostgreSQL、Python 和 NVIDIA GPU Operator 构建的架构——运行在我们实验室中一台配备两张 NVIDIA RTX PRO 6000 Blackwell GPU 的 Cisco UCS C845A 上。这是一个实用的起点，解决了调度、多租户、成本建模以及保持 GPU 基础设施可靠性的 Day-2 运营现实问题。

这并不像看起来那么令人生畏。工具已经成熟，你可以使用熟悉的构建块组装出类似云的工作流：从浏览器预订 GPU 容量，进入满载的 ML 工作区，并在几秒钟内启动推理服务。区别在于它运行在哪里——在你拥有的基础设施上，在你的运营控制下，数据永远不会离开你的四面墙。在实践中，入门门槛通常比领导者预期的要低。

将其扩展到多个 Cisco AI Pod，调度平面、协调器模式和隔离模型可以直接沿用。基础是一样的。

如果你正在处理这些相同的决策——如何调度 GPU、如何隔离租户、如何为本地 AI 基础设施构建商业案例，我非常欢迎与你交流。
