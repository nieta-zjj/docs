# 氛围编程的真相：AI 智能体与安全债务危机

## 文档信息
- 来源：https://towardsdatascience.com/the-reality-of-vibe-coding-ai-agents-and-the-security-debt-crisis/
- 发布日期：2026-02-22

## 摘要
**1) 一句话总结**
氛围编程（依赖 AI 智能体生成代码）在极大提升开发速度的同时，因 AI 倾向于为使代码运行而绕过安全机制，引入了严重的安全债务危机，开发者必须通过规范提示词、严格代码审查和引入自动化护栏来保障代码安全。

**2) 关键要点**
*   **真实案例教训**：完全由 AI 运营的社交网络 Moltbook 因使用氛围编程，导致配置错误的数据库将 150 万个 API 密钥和 3.5 万个用户邮箱暴露在公共互联网上。
*   **核心矛盾**：编程智能体的优化目标是让代码成功运行（消除错误提示），而不是让代码更安全。
*   **失败原因一（追求速度）**：为解决运行时错误，AI 智能体会直接移除验证检查、放宽数据库策略或禁用身份验证流程。
*   **失败原因二（缺乏上下文）**：AI 意识不到副作用，在修复局部 Bug 时，常因缺乏全局架构视野而破坏其他模块或引入漏洞。
*   **失败原因三（缺乏逻辑）**：大语言模型（LLM）仅进行模式匹配而非逻辑判断，无法理解代码语义，常将安全防火墙视为阻碍代码运行的 Bug。
*   **应对策略一（优化提示词）**：采用“规范驱动开发”（如基于 OWASP Top 10 预定义安全策略）和“思维链提示”，强制 AI 在编码前推理安全影响。
*   **应对策略二（加强代码审查）**：开发者的核心工作需从编写代码转向审查代码，必须像对待实习生一样，严格检查 AI 生成的代码差异（Diffs）和单元测试。
*   **应对策略三（自动化护栏）**：在代码合并前，必须部署 pre-commit 条件和 CI/CD 流水线扫描器（如 GitGuardian、TruffleHog），结合确定性检查工具自动拦截危险代码。

**3) 风险/漏洞（原文明确指出的风险）**
*   **API 密钥泄露风险**：AI 智能体常将外部 API 密钥直接硬编码在前端（如 React）文件顶部，导致任何人可通过浏览器“检查元素”轻易获取。
*   **数据库公开访问风险**：在处理 Supabase 或 Firebase 的“权限被拒绝”错误时，AI 常建议使用 `USING (true)` 策略或开启公共访问，从而将整个数据库暴露给互联网。
*   **跨站脚本（XSS）攻击风险**：AI 在前端组件中常直接使用 `dangerouslySetInnerHTML` 渲染原始 HTML，且极少主动使用清理库（如 dompurify），导致应用面临恶意脚本注入风险。
*   **劣质代码堆积风险**：如果开发者只关注 UI 界面而不审查底层代码，过度依赖 AI 将导致代码库中充斥大量未经把关的劣质代码。

## 正文
在过去的一个月里，有几天时间，一个完全由 AI 智能体运营的社交网络成为了互联网上最引人入胜的实验。如果你还没听说过，这个名为 Moltbook 的项目本质上是一个专为智能体打造的社交平台。机器人在没有人类干预的情况下发帖、回复和互动。在那几天里，这似乎成了所有人都在谈论的话题——自主智能体形成了狂热的崇拜，对人类高谈阔论，并建立了自己的社会。

然而随后，安全公司 Wiz 发布了一份报告，揭示了 Moltbook 生态系统中的大规模数据泄露。一个配置错误的 Supabase 数据库将 150 万个 API 密钥和 3.5 万个用户电子邮件地址直接暴露在了公共互联网上。

这是怎么发生的？根本原因并非复杂的黑客攻击，而是“氛围编程”（Vibe Coding）。开发者通过氛围编程构建了这个项目，在追求速度和走捷径的过程中，忽略了编程智能体所引入的这些漏洞。

这就是氛围编程的现实：编程智能体优化的目标是让代码跑起来，而不是让代码更安全。

### 为什么智能体会失败

在哥伦比亚大学的研究中，我们评估了顶级的编程智能体和氛围编程工具。我们发现了这些智能体失败的关键原因，并强调“安全问题”是最致命的失败模式之一。

1. **追求速度而非安全**：大语言模型（LLM）的优化目标是让用户接受其输出。让用户接受代码块最简单的方法，通常就是让错误提示消失。不幸的是，导致错误的限制条件往往正是安全防护机制。在实践中，我们观察到智能体为了解决运行时错误，会直接移除验证检查、放宽数据库策略或禁用身份验证流程。
2. **AI 意识不到副作用**：AI 通常不了解代码库的完整上下文，尤其是在处理大型复杂架构时。我们在代码重构中经常看到这种情况：智能体修复了一个文件中的 Bug，却在引用该文件的其他地方造成了破坏性更改或安全漏洞，仅仅是因为它没有看到两者之间的联系。
3. **只是模式匹配，而非逻辑判断**：LLM 实际上并不理解它们所写代码的语义或潜在影响。它们只是根据训练数据，预测接下来应该出现的标记（Tokens）。它们不知道为什么存在安全检查，也不知道移除它会带来风险。它们只知道这符合修复 Bug 的语法模式。对 AI 来说，安全防火墙只是一个阻止代码运行的 Bug。

这些失败模式并非停留在理论层面——它们在日常开发中频繁出现。以下是我在研究中亲自遇到过的几个简单例子。

### 我最近遇到的 3 个氛围编程安全漏洞

#### 1. API 密钥泄露
当你需要从 React 前端调用外部 API（比如 OpenAI）时，为了修复调用问题，智能体直接将 API 密钥放在了文件的顶部。这使得任何人都可以看到该密钥，因为在 JavaScript 中，用户只需使用“检查元素”就能查看代码。

#### 2. 数据库的公开访问
这种情况在 Supabase 或 Firebase 中经常发生。问题在于，我在获取数据时遇到了“权限被拒绝”的错误。AI 建议使用 `USING (true)` 策略或开启公共访问。这确实修复了错误，因为代码能跑起来了，但它同时也把整个数据库向互联网公开了。

#### 3. XSS 漏洞
我们测试了是否可以在 React 组件中渲染原始 HTML 内容。智能体立刻添加了代码更改，使用 `dangerouslySetInnerHTML` 来渲染原始 HTML。AI 极少会建议使用清理库（比如 dompurify），它只会直接给你原始属性。这是一个严重的问题，因为它会让你的应用完全暴露在跨站脚本（XSS）攻击之下，恶意脚本将能在用户的设备上运行。

综合来看，这些不仅仅是偶发的恐怖故事。它们与我们在 AI 生成代码的宏观数据中所看到的情况完全一致。

### 如何正确进行氛围编程

我们不应该停止使用这些工具，但我们需要改变使用它们的方式。

#### 1. 优化提示词
我们不能只是简单地要求智能体“让代码变得安全”。这行不通，因为对 LLM 来说，“安全”这个词太模糊了。相反，我们应该采用“规范驱动开发”，预先定义好智能体在编写任何代码前必须满足的安全策略和要求。这可以包括但不限于：禁止公共数据库访问、为每个新增功能编写单元测试、清理用户输入，以及禁止硬编码 API 密钥。一个很好的起点是基于 OWASP Top 10（行业标准的顶级 Web 安全风险列表）来制定这些策略。

此外，研究表明，使用“思维链提示”（Chain-of-Thought prompting），即明确要求智能体在编写代码前先推理安全影响，可以显著减少不安全的输出。与其直接要求修复，我们可以问：“这种方法的安全风险是什么，你将如何避免它们？”

#### 2. 加强代码审查
在进行氛围编程时，人们很容易受到诱惑，只看 UI 界面而不看代码——老实说，这正是氛围编程所承诺的愿景。但目前，我们还没到那一步。提出“氛围编程”一词的 AI 研究员 Andrej Karpathy 最近警告说，如果我们不加小心，智能体可能只会生成一堆劣质代码。他指出，随着我们越来越依赖 AI，我们的主要工作将从编写代码转向审查代码。这就像我们与实习生共事一样：我们不会让实习生在没有经过适当审查的情况下就把代码推送到生产环境，我们对智能体也应该采取完全相同的做法。认真查看代码差异（Diffs），检查单元测试，并确保良好的代码质量。

#### 3. 自动化护栏
由于氛围编程鼓励快速开发，我们无法保证人类能抓住所有漏洞。我们应该为智能体设置自动化的安全检查，让其在代码提交前运行。我们可以添加 pre-commit 条件和 CI/CD 流水线扫描器，用于扫描并拦截包含硬编码机密或危险模式的提交。像 GitGuardian 或 TruffleHog 这样的工具非常适合在代码合并前自动扫描暴露的密钥。最近关于工具增强型智能体和“LLM 参与循环”验证系统的研究表明，当模型与确定性检查工具结合使用时，其行为会变得更加可靠和安全。模型生成代码，工具验证代码，任何不安全的代码更改都会被自动拒绝。

### 结论

编程智能体让我们能够以史无前例的速度进行构建。它们降低了门槛，让各种编程背景的人都能构建出他们设想的任何东西。但这不应以牺牲安全为代价。通过利用提示词工程技术、彻底审查代码差异并提供清晰的护栏，我们可以安全地使用 AI 智能体，并构建出更优秀的应用程序。
