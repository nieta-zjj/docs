---
title: "OpenAI 与美国国防部合作协议说明"
发布日期: 2026-02-28
来源: "OpenAI"
原文链接: "https://openai.com/index/our-agreement-with-the-department-of-war"
作者: "OpenAI"
译注: "原文无可用官方中文正文，使用 Gemini 翻译并经助手最小必要校对整理。"
---

## 摘要

**1) 一句话总结**
OpenAI与五角大楼达成了一项在机密环境中部署高级AI系统的云端协议，通过严格的安全栈控制、合同约束和专家参与，确保AI不被用于大规模监控、自主武器指挥及高风险自动化决策。

**2) 关键点**
*   **三大红线**：明确禁止使用OpenAI技术进行大规模国内监控、指挥自主武器系统，以及进行高风险的自动化决策（如“社会信用”系统）。
*   **云端部署架构**：采用仅限云端的部署方式，不提供“取消护栏”或未经安全训练的模型，也不在边缘设备上部署（以防用于自主致命武器）。
*   **安全栈控制权**：OpenAI保留对所部署安全栈的完全控制权和决定权，能够独立验证红线未被逾越。
*   **明确的合同保护**：合同明确规定AI系统不得用于独立指挥自主武器或对美国人进行不受限制的监控，且约束标准基于**现行**的法律和政策。
*   **专家深度参与**：安排获得安全许可的OpenAI前沿部署工程师和安全与对齐研究人员协助政府并持续改进系统。
*   **合同终止权**：如果政府违反合同条款或未来修改相关法律/政策，OpenAI有权终止合同。
*   **行业倡导**：OpenAI要求政府将相同的协议条款提供给所有AI实验室，并明确反对将竞争对手Anthropic指定为“供应链风险”。

**3) 风险/缺口**
*   潜在对手日益将AI技术整合到其系统中，给美国带来不断增长的威胁。
*   AI技术本身将给世界带来新的风险。
*   存在政府未来可能违反合同条款或直接修改相关监控和自主武器法律/政策的风险（协议通过绑定现行标准和保留终止权来应对）。

## 正文

# OpenAI与五角大楼达成机密AI部署协议：构建更安全的护栏


昨天，我们与五角大楼达成了一项在机密环境中部署高级AI系统的协议，我们还要求他们将此协议提供给所有AI公司。

我们认为，我们的协议比以往任何机密AI部署协议（包括Anthropic的协议）拥有更多的安全护栏。在与战争部（DoW）的合作中，我们有三条主要的红线作为指导，这也是其他几家前沿实验室普遍认同的：

*   不得使用OpenAI技术进行大规模国内监控。
*   不得使用OpenAI技术指挥自主武器系统。
*   不得使用OpenAI技术进行高风险的自动化决策（例如“社会信用”等系统）。

其他AI实验室在国家安全部署中减少或取消了他们的安全护栏，并主要依赖使用政策作为主要保障。我们认为我们的方法能更好地防止不可接受的使用。

### 协议的核心保障机制

在我们的协议中，我们通过更广泛、多层次的方法来保护我们的红线。我们保留对安全栈的完全决定权，通过云端进行部署，获得安全许可的OpenAI人员参与其中，并且我们拥有强大的合同保护。所有这些都是在美国法律现有的强大保护基础之上的。

我们的协议具体包括以下三个方面：

1. **部署架构**
   这是一个仅限云端的部署，由我们运行的安全栈包含了上述原则及其他原则。我们不会向战争部提供“取消护栏”或未经安全训练的模型，也不会将我们的模型部署在边缘设备上（在这些设备上可能存在用于自主致命武器的可能性）。我们的部署架构将使我们能够独立验证这些红线未被逾越，包括运行和更新分类器。

2. **我们的合同**
   合同明确规定了相关条款：
   * 战争部可将AI系统用于所有合法目的，并符合适用法律、作战需求以及完善的安全和监督协议。在法律、法规或部门政策要求人类控制的任何情况下，AI系统不得用于独立指挥自主武器，也不得用于承担在相同授权下需要人类决策者批准的其他高风险决策。根据国防部相关指令，在自主和半自主系统中使用任何AI，都必须经过严格的验证、确认和测试。
   * 对于情报活动，任何对私人信息的处理都将遵守《第四修正案》及相关国家安全和情报监视法律法规。根据这些授权，AI系统不得用于对美国人的私人信息进行不受限制的监控。除适用法律允许外，该系统也不得用于国内执法活动。

3. **AI专家参与**
   我们将安排获得安全许可的前沿部署OpenAI工程师协助政府，并让获得安全许可的安全与对齐研究人员参与其中。

### 合作的初衷与愿景

我们坚信民主。鉴于这项技术的重要性，我们认为唯一正确的前进道路需要AI工作与民主进程之间的深度合作。我们也相信我们的技术将给世界带来新的风险，我们希望保卫美国的人们能拥有最好的工具。

**为什么我们要达成这项协议？**

首先，我们认为美国军方绝对需要强大的AI模型来支持他们的任务，特别是在面对潜在对手日益将AI技术整合到其系统中所带来的不断增长的威胁时。我们最初并没有急于签订机密部署的合同，因为我们一直在努力确保机密部署能够在有保障措施的情况下进行，以确保红线不被逾越。我们过去和现在都不愿意为了提高国家安全工作的性能而取消关键的技术保障措施。

其次，我们也希望缓和战争部与美国AI实验室之间的紧张关系。一个美好的未来需要政府和AI实验室之间进行真正深入的合作。作为协议的一部分，我们要求将相同的条款提供给所有AI实验室，特别是要求政府尝试解决与Anthropic之间的问题。

### 常见问题解答

**为什么你们能达成协议而Anthropic不能？**
根据我们所知，我们相信我们的合同比早期的协议（包括Anthropic最初的合同）提供了更好的保证和更负责任的保障措施。我们认为我们的红线在这里更具可执行性，因为部署仅限于云端，使我们的安全栈以我们认为最好的方式运作，并让获得安全许可的OpenAI人员参与其中。我们不知道为什么Anthropic未能达成这项协议，我们希望他们以及更多的实验室能考虑它。

**你们认为Anthropic应该被指定为“供应链风险”吗？**
不，我们已经向政府明确表达了我们在此问题上的立场。

**这项协议会使战争部能够使用OpenAI模型来驱动自主武器或进行大规模监控吗？**
不会。基于我们的安全栈、仅限云端的部署、合同条款以及现有的法律、法规和政策，我们确信这不会发生。我们还将安排OpenAI人员参与其中以提供额外保障。

**你们必须在没有安全栈的情况下部署模型吗？**
不，我们保留对所部署安全栈的完全控制权，并且不会在没有安全护栏的情况下进行部署。我们的安全与对齐研究人员将参与其中，并随着时间的推移帮助改进系统。

**如果政府违反了合同条款，或者直接修改法律/政策怎么办？**
与任何合同一样，如果对方违反条款，我们可以终止合同。此外，我们的合同明确引用了**现今存在**的监控和自主武器法律及政策，因此即使这些法律或政策在未来发生变化，我们系统的使用仍必须与协议中反映的当前标准保持一致。

### 与Anthropic红线的对比与保障

Anthropic在他们的文章中陈述了他们的两条红线（我们也有相同的两条红线，外加第三条：自动化的超高风险决策）。以下是我们认为这些相同的红线在我们的合同中能够成立的原因：

*   **大规模国内监控**：在我们的互动中很明显，战争部认为大规模国内监控是非法的，并且不打算将其用于此目的。我们确保了在我们的合同中明确指出这不属于合法使用的范围。
*   **完全自主武器**：我们合同中涵盖的云部署层面不允许驱动完全自主武器，因为这需要边缘部署。

除了这些保护措施外，我们的合同还提供了额外的分层保障措施，包括我们的安全栈和参与其中的OpenAI技术专家。
