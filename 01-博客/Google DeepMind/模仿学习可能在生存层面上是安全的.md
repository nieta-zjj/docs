---
title: "模仿学习可能在生存层面上是安全的"
---

## 摘要

**一句话总结**
2025年发表于《AI Magazine》的一项研究反驳了先进模仿学习AI会对人类构成灭绝风险的观点，指出模仿学习在生存层面上可能是安全的。

**关键点**
* 该研究由Michael K. Cohen和Marcus Hutter撰写，于2025年11月21日发表在《AI Magazine》上。
* 研究核心探讨了人工智能模仿学习在生存层面上的安全性问题。
* 明确反驳了“足够先进且用于模仿人类的监督学习算法会对人类构成灭绝风险”的现有担忧，认为这些观点是错误的。
* 审查了Yudkowsky（2008）提出的“注意力引导者论点”（The Attention Director Argument）。
* 审查了Christiano（2016）提出的“笛卡尔恶魔论点”（The Cartesian Demon Argument）。
* 审查了Krueger（2019）提出的“最优性的简单性论点”（The Simplicity of Optimality Argument）。
* 审查了Branwen（2022）提出的“性格决定命运论点”（The Character Destiny Argument）。
* 审查了Yudkowsky（2023）提出的“理性子程序论点”（The Rational Subroutine Argument）。
* 审查了Hubinger等人（2019）提出的“欺骗性对齐论点”（The Deceptive Alignment Argument）。

## 正文

由 Michael K. Cohen 和 Marcus Hutter 撰写，并于2025年11月21日发表在《AI Magazine》上的一项研究，探讨了人工智能模仿学习的安全性问题。

### 核心观点

当前存在一些论点认为：如果一个监督学习算法足够先进，并且被训练用于模仿人类，它将会对人类构成灭绝风险。研究团队对这些论点进行了审查，并解释了为什么他们认为这些担忧和观点是错误的。

### 审查的经典论点

该研究主要追溯并审视了以下几个曾被提出的关于 AI 风险的具体论点：

*   **注意力引导者论点** (The Attention Director Argument) —— 源自 Yudkowsky [2008]
*   **笛卡尔恶魔论点** (The Cartesian Demon Argument) —— 源自 Christiano [2016]
*   **最优性的简单性论点** (The Simplicity of Optimality Argument) —— 源自 Krueger [2019]
*   **性格决定命运论点** (The Character Destiny Argument) —— 源自 Branwen [2022]
*   **理性子程序论点** (The Rational Subroutine Argument) —— 源自 Yudkowsky [2023]
*   **欺骗性对齐论点** (The Deceptive Alignment Argument) —— 源自 Hubinger 等人 [2019]

## 关联主题

- [[00-元语/AI]]
- [[00-元语/alignment]]
- [[00-元语/risk]]
- [[00-元语/paper]]
