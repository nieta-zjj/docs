---
title: "Genie 2：大规模基础3D世界模型"
---

## 摘要

### 1) 一句话总结
Genie 2 是一个能够通过单张图像生成多样化、动作可控且可玩的3D环境的大规模基础世界模型，旨在为具身智能体提供无限的训练与评估场景。

### 2) 关键要点
*   **核心交互**：只需提供一张提示图像（可由 Imagen 3 生成），人类或AI智能体即可通过键盘和鼠标在生成的3D环境中进行游玩和交互。
*   **生成时长**：模型能够生成长达1分钟的连贯世界，大多数生成示例的持续时间在10到20秒之间。
*   **涌现能力**：在海量视频数据上训练后，展现出动作控制、长视野记忆、反事实体验生成、复杂角色动画、NPC建模以及精准物理特效（如水流、重力、光照）等能力。
*   **强大的泛化性**：具备分布外泛化（OOD）能力，能够将真实世界照片、概念艺术或手绘草图转化为完全交互式的3D环境，支持快速原型设计。
*   **赋能智能体研究**：可为具身智能体（如 SIMA）生成训练期间未见过的评估任务；同时也可利用智能体执行指令（如“转过身”）来反向评估 Genie 2 生成环境的连贯性。
*   **模型架构**：采用自回归的潜在扩散模型（Latent Diffusion Model），结合自编码器和带有因果掩码的大型 Transformer 动力学模型。
*   **动作可控性**：在推理阶段，使用无分类器引导（Classifier-free guidance）技术来提高动作控制的准确性。
*   **版本差异**：当前高质量演示样本由未蒸馏的基础模型生成；另有蒸馏版本可实现实时游玩，但输出质量较低。

### 3) 风险与不足
*   **技术成熟度**：该研究仍处于早期阶段，模型在通用性和连贯性等环境生成能力上仍有很大的提升空间。
*   **性能权衡**：为了实现实时游玩而使用的蒸馏版本模型，会导致输出质量下降。
*   **不可控的异常表现**：模型偶尔会生成意料之外的画面（例如在无动作输入时出现幽灵，或角色面对滑雪板时选择跑酷而非滑雪）。

## 正文

今天，我们正式推出 Genie 2——一个基础世界模型。它能够生成种类繁多、动作可控且可玩的3D环境，用于训练和评估具身智能体（Embodied Agents）。只需提供一张提示图像，人类或AI智能体就可以通过键盘和鼠标在生成的环境中进行游玩。

游戏在人工智能（AI）研究领域一直扮演着关键角色。它们具有吸引力、独特的挑战性以及可衡量的进步空间，是安全测试和提升AI能力的理想环境。从早期的Atari游戏研究，到AlphaGo和AlphaStar的突破，再到与游戏开发者合作的通用智能体（如SIMA），游戏始终处于我们研究的中心。然而，训练更通用的具身智能体，传统上一直受限于缺乏足够丰富和多样化的训练环境。

Genie 2 的出现，使得未来的智能体能够在无限量的新颖世界课程中进行训练和评估。同时，这项研究也为交互式体验的原型设计开辟了全新的创意工作流。

### 基础世界模型的涌现能力

在此之前，世界模型大多局限于对狭窄领域的建模。在 Genie 1 中，我们引入了生成多种2D世界的方法；而今天的 Genie 2 则在通用性上实现了重大飞跃，能够生成极其丰富多样的3D世界。

作为一个世界模型，Genie 2 能够模拟虚拟世界以及采取任何动作（如跳跃、游泳等）所带来的后果。它在海量视频数据集上进行了训练，并与其他生成式模型一样，在规模化后展现出了多种涌现能力，例如物体交互、复杂的角色动画、物理规律，以及对其他智能体行为进行建模和预测的能力。

用户可以通过 GDM 最先进的文本到图像模型 Imagen 3 生成单张图像作为提示。这意味着任何人都可以用文字描述他们想要的世界，选择最喜欢的渲染效果，然后步入并与这个新创建的世界互动（或者让AI智能体在其中进行训练和评估）。在每一步中，人类或智能体输入键盘和鼠标动作，Genie 2 就会模拟出下一个观察画面。Genie 2 能够生成长达一分钟的连贯世界，大多数示例的持续时间在10到20秒之间。

Genie 2 具备以下核心能力：

*   **动作控制：** 模型能智能响应键盘按键动作，准确识别并移动角色（例如，模型能理解方向键应该移动机器人，而不是树木或云彩）。
*   **生成反事实体验：** 从同一个起始帧开始，根据人类玩家采取的不同动作，可以生成截然不同的轨迹，这为训练智能体提供了模拟反事实体验的可能。
*   **长视野记忆：** 能够记住离开视线范围的世界部分，并在它们重新进入视野时准确地将其渲染出来。
*   **长视频与新内容生成：** 能够即时生成合理的新内容，并维持长达一分钟的连贯世界。
*   **多样化环境与3D结构：** 支持创建不同的视角（如第一人称、等距视角或第三人称驾驶视角），并学会了创建复杂的3D视觉场景。
*   **物体可供性与交互：** 能够对各种物体交互进行建模，例如戳破气球、开门和射击爆炸桶。
*   **角色动画与NPC：** 学会了如何让不同类型的角色执行各种活动，并能对其他智能体（NPC）及其复杂的交互进行建模。
*   **物理与环境特效：** 能够精准模拟水流、烟雾、重力、点光源与平行光、反射、泛光以及彩色光照等物理和视觉效果。
*   **真实图像驱动：** 即使输入真实世界的照片，Genie 2 也能进行模拟，例如展现风中摇曳的草或河流中流动的水。

### 支持快速原型设计

Genie 2 使得快速构建多样化交互体验的原型变得异常简单，研究人员可以借此快速实验新颖的环境，以训练和测试具身AI智能体。

例如，我们可以输入不同的图像，测试 Genie 2 如何模拟驾驶纸飞机、龙、老鹰或降落伞的区别，并检验其动画化不同化身的能力。得益于 Genie 2 的分布外泛化（OOD）能力，概念艺术和手绘草图也能被转化为完全交互式的环境。这使得艺术家和设计师能够快速制作原型，从而启动环境设计的创意过程，进一步加速研究。

### AI智能体在世界模型中的应用

通过使用 Genie 2 快速创建丰富多样的环境，研究人员可以生成智能体在训练期间从未见过的评估任务。

例如，我们与游戏开发者合作开发的 SIMA 智能体，可以遵循自然语言指令在 3D 游戏世界中完成任务。我们利用 Genie 2 生成了一个包含红蓝两扇门的 3D 环境，并指示 SIMA 智能体分别打开它们。在这个过程中，SIMA 通过键盘和鼠标输入控制化身，而 Genie 2 则负责生成游戏画面。

同时，我们也可以利用 SIMA 来评估 Genie 2 的能力。例如，通过指示 SIMA “转过身”或“走到房子后面”，来测试 Genie 2 生成连贯环境的能力。

尽管这项研究仍处于早期阶段，智能体和环境生成能力都有很大的提升空间，但我们相信，Genie 2 是解决安全训练具身智能体这一结构性问题的有效途径，并能提供迈向通用人工智能（AGI）所需的广度和通用性。

### 扩散世界模型架构

Genie 2 是一个自回归的潜在扩散模型（Latent Diffusion Model），在大型视频数据集上进行训练。视频帧首先通过自编码器（Autoencoder）处理，随后这些潜在帧被输入到一个大型 Transformer 动力学模型中，该模型使用了类似于大型语言模型的因果掩码（Causal Mask）进行训练。

在推理阶段，Genie 2 可以以自回归的方式进行采样，逐帧接收单个动作和过去的潜在帧。我们使用了无分类器引导（Classifier-free guidance）技术来提高动作的可控性。

为了展示模型的潜力，目前演示的样本均由未蒸馏的基础模型生成。我们也可以运行一个蒸馏版本的模型来实现实时游玩，但输出质量会有所降低。

### 负责任的开发与趣味花絮

Genie 2 展示了基础世界模型在创建多样化 3D 环境和加速智能体研究方面的巨大潜力。这一研究方向尚处于起步阶段，我们期待在通用性和连贯性方面继续提升 Genie 的世界生成能力。与 SIMA 一样，我们的研究旨在构建更通用的 AI 系统和智能体，使其能够理解并安全地执行广泛的任务，从而在网络和现实世界中为人类提供帮助。

在开发过程中，我们也发现了一些有趣的“花絮”：例如在没有任何动作输入的情况下，花园里突然出现了一个幽灵；或者角色在面对滑雪板时，却更倾向于表演跑酷。这些意料之外的表现也为世界模型的探索增添了独特的趣味。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/Agent]]
- [[00-元语/game]]
- [[00-元语/multimodal]]
