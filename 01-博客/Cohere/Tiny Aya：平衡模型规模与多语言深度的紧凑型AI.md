# Tiny Aya：平衡模型规模与多语言深度的紧凑型AI

## 文档信息
- 来源：https://cohere.com/research/papers/tiny-aya-bridging-scale-and-multilingual-depth-2026-02-17
- 发布日期：2026-02-17

## 摘要
**1) 一句话总结**
2026年2月17日发布的Tiny Aya是一款拥有33.5亿参数的紧凑型AI模型，通过创新训练方法在70种语言任务中达到最先进水平，实现了模型规模与多语言深度的平衡及高效部署。

**2) 关键点**
* **发布时间**：该研究成果于2026年2月17日正式发布。
* **模型规模**：参数量为33.5亿（3.35B），定位为紧凑型多语言模型。
* **技术创新**：采用了创新的训练方法，并推出了针对特定区域优化的模型变体。
* **语言覆盖与性能**：在多达70种语言的翻译和理解任务中，均达到了最先进（State-of-the-art）水平。
* **部署优势**：为多语言人工智能的实际应用与落地部署提供了高效且平衡的路径。
* **研究团队**：由Alejandro R. Salamanca、Sara Hooker、Aidan Gomez等20余位研究人员共同参与研发。
* **相关前沿**：该领域还涉及SimMerge、Best of N优化以及EAGER（面向自适应推理阶段扩展的熵感知生成）等相关前沿研究工作。

## 正文
2026年2月17日，一项名为 **Tiny Aya** 的最新研究正式发布，旨在为模型规模与多语言深度之间搭建桥梁。

### 核心突破与优势

Tiny Aya 是一款参数量为 33.5 亿（3.35B）的紧凑型多语言模型。通过创新的训练方法以及针对特定区域优化的变体，该模型实现了以下突破：

* **卓越的多语言能力**：在多达 70 种语言的翻译和理解任务中，均达到了最先进（State-of-the-art）的水平。
* **高效的实际部署**：为多语言人工智能的实际应用与部署提供了一条高效且平衡的路径。

### 研究团队

该项目由众多研究人员共同完成，作者团队包括：Alejandro R. Salamanca, Diana Abagyan, Daniel D’souza, Ammar Khairi, David Mora, Saurabh Dash, Viraat Aryabumi, Sara Rajaee, Mehrnaz Mofakhami, Ananya Sahu, Thomas Euyang, Brittawnya Prince, Madeline Smith, Hangyu Lin, Acyr Locatelli, Sara Hooker, Tom Kocmi, Aidan Gomez, Ivan Zhang, Phil Blunsom, Nick Frosst, Joelle Pineau, Beyza Ermis, Ahmet Üstün, Julia Kreutzer 以及 Marzieh Fadaee。

### 相关前沿研究

围绕该领域，还有以下几项相关的研究工作值得进一步关注：

* **SimMerge**：从相似性信号中学习选择合并操作符（Learning to Select Merge Operators from Similarity Signals）
* **Making, not Taking, the Best of N**
* **EAGER**：面向自适应推理阶段扩展的熵感知生成（Entropy-Aware Generation for Adaptive Inference-Time Scaling）
