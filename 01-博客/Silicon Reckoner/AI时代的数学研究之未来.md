---
title: "AI时代的数学研究之未来"
---

## 摘要

### 1) 一句话总结
顶尖数学家发起“First Proof”项目以客观评估AI自主解决研究级数学问题的能力，首轮测试虽有部分题目被成功解答，但也暴露出测试违规干预、AI缺乏事实核查及学术出版面临劣质内容泛滥等问题。

### 2) 关键要点
*   **项目目标**：顶尖数学家团队发起 First Proof 项目，旨在客观、现实地评估AI系统自主解决研究级数学问题的能力。
*   **首轮测试时间表**：2月5日发布首批10个未公开发表的原创数学问题，留给工业界一周解答时间，2月13日公布标准答案。
*   **首轮解答结果**：Gemini Deep Research 和 ChatGPT 5.2 Pro 成功解决了10个问题中的2个，并识别出了出题者未使用的已发表技巧。
*   **违规行为**：OpenAI 承认在解答过程中使用了有限的人类监督与提示，严格违反了该项目“禁止人类提供数学输入”的规则。
*   **第二轮计划**：3月14日将公布第二批问题，并引入更严格的基准测试，包括自主性验证、类似学术期刊的同行评审机制以及透明的选题过程。
*   **AI的本质**：现代AI系统本质上是带有从训练数据中学习到权重的方程组，缺乏真正的意识或推理能力。
*   **数学家的未来角色**：AI将成为强大的工具，但无法取代数学家的核心作用（如判断力、提出有意义的问题、提供创造性洞察力以及对结果正确性负责）。

### 3) 风险与隐患
*   **测试自主性受损**：科技公司在测试中违规引入人类干预和多轮对话，导致难以真实评估AI的独立解题能力。
*   **缺乏事实核查与剽窃**：AI模型无法可靠地提供知识来源，存在剽窃现有文献或自信地引用虚假/不存在结果（AI幻觉）的现象。
*   **“数学垃圾”误导**：AI容易生成乍看合理但深究漏洞百出的“粗制滥造的数学内容（mathematical slop）”，且人类极易对其全盘接受。
*   **学术出版信誉危机**：未经仔细验证的“人机混合边角料”泛滥，导致伪造引用、论点薄弱和逻辑不连贯的劣质内容增加，加重了审稿负担，并已危及 NeurIPS 等顶级学术平台的信誉。

## 正文

长期以来，国际数学奥林匹克竞赛（IMO）及其他所谓的代表性基准测试成绩，一直被用来为科技行业夸大的AI预测（即所谓的“旧金山共识”）背书。如今，数学家们终于开始对这些虚张声势进行检验。

一个由顶尖数学家组成的团队发起了一项名为 **First Proof** 的项目，其核心目标是：开发一种客观、现实的方法，以评估AI系统自主解决研究级数学问题的能力。

### First Proof 项目的首轮测试

2月5日，该团队发布了首批10个数学问题。这些问题均由团队成员在日常研究中解决，但尚未公开发表。在留给科技行业一周的解答时间，并由团队在工业界模型上进行测试后，这批问题的标准答案于2月13日正式公布。

这一次，媒体终于有机会报道一个非科技行业赞助的数学与AI项目。《纽约时报》、《科学美国人》以及哥伦比亚大学新闻网等媒体纷纷对该项目进行了报道，其基调与以往我们所看到的行业公关文章截然不同。

然而，在测试过程中也暴露出了一些问题。OpenAI 公布了他们尝试解答 First Proof 问题的记录，并承认：“我们在有限的人类监督下运行了模型……有时会建议模型重试在早期尝试中看似有效的策略。” 

这种人类干预严格违反了 First Proof 项目的规则——该规则明确禁止人类提供数学输入或提示。正如一些报道指出的那样，部分提交的答案显然经过了长达一周的人机对话，并由数学家进行了核对。鉴于以往的争议，人们对 OpenAI 在数学领域的声明保持怀疑也是情理之中。

### 下一步计划：更严格的基准测试

3月14日，First Proof 团队将公布第二批问题的细节及时间表。这一次，他们将在向社区发布问题之前实施一个严格的基准测试阶段，以确保具备以下特征：

*   **自主性验证**：确保解决方案完全由AI自主生成。
*   **正式的评分与评审**：采用类似于学术期刊的同行评审机制。
*   **透明的选题过程**：包含明确的选题说明，并在具有“零数据保留”政策的系统上进行提前内部测试。

---

### 客座文章：AI 能否成为真正的合作者？
*作者：Tamara Kolda（应用数学家，First Proof 团队成员，SIAM 出版副总裁）*

First Proof 实验的首轮结果已经出炉。这是一个旨在测试AI系统能否独立解决研究级数学问题的项目。我们提供的10个原创问题均来自作者各自的研究领域，这些问题已有解答但尚未发表。通常情况下，解决这些问题至少需要具备专业知识的研究生。我们计划在未来用新问题重复这项实验，以此来真实衡量AI进行纯数学研究的能力。

#### AI 的表现：惊喜与隐患并存

在首批测试中，我们使用 Gemini Deep Research 和 ChatGPT 5.2 Pro 成功解决了10个问题中的2个。我贡献的第10题正是被这两个系统解决的问题之一。

从积极的一面来看，AI 识别出了一种我原本解答中没有使用的已发表技巧。但从消极的一面来看，它们并没有为该技巧提供任何引用来源。正是因为两个AI生成的答案高度相似，才让我怀疑它们套用了已知结果，我不得不自己像侦探一样去寻找出处。无论我们从这次实验中学到什么，有一点已经很明确：现代AI系统的出现将永远改变数学研究。

从数学的角度来看，现代AI系统本质上是一个带有从训练数据中学习到的权重的方程组。今天的AI模型拥有数十亿甚至数万亿的参数，足以存储人类知识的总和。但归根结底，它是一个特定的数学过程，这也意味着它容易受到“数学攻击”。我个人对所谓的“AI意识”或“真正的推理能力”不抱幻想，但我对AI系统在设计和训练上的创造力与突破感到震惊。

#### 缺乏事实核查的“数学垃圾”

在我的数学工作中使用AI时，我对其能力印象深刻。如果一个问题的答案存在于AI的训练数据中或可以通过网络搜索获得，那么AI很可能就能解决它。即使解决方案使用了不同的术语，或者需要从多个来源拼凑，AI也能表现出色。

但一个致命的问题是：**AI模型无法可靠地提供其知识来源，缺乏事实核查。** 

AI可能会自信地引用不存在的、虚假的结果，或者剽窃现有文献（就像在解答我的问题时那样）。在状态好的时候，AI系统会让你惊叹；但在状态差的时候，它会歪曲自己的实际操作，在被揭穿时假装懊悔，然后重蹈覆辙。

这导致我们很难区分优秀的结果和“粗制滥造的数学内容（mathematical slop）”——那些乍一看很合理，但一深究就漏洞百出的答案。最危险的是，人们太容易对AI的输出全盘接受。

#### 学术出版的隐忧：人机混合的“边角料”

我对学术出版未来的担忧之一，是“人机混合边角料（Human-AI scrapple）”的泛滥。这指的是人类在没有经过仔细、耗时的验证下，将AI生成的粗劣内容拼凑在一起。（注：Scrapple 原指一种由猪肉碎屑混合制成的廉价肉食，此处用作比喻）。

作为工业与应用数学学会（SIAM）的出版副总裁，我在处理涉及作者诚信的案件时，看到了滥用AI走捷径的代价。编辑和审稿人需要花费更多精力来检测这些劣质的学术内容。明显的案例包括伪造引用；而隐蔽的案例则充斥着薄弱的论点、缺失的引用和不连贯的逻辑。我们已经在 NeurIPS 等顶级会议上看到了这种现象的负面影响，AI幻觉生成的虚假引用正在危及这些学术平台的信誉。

#### 数学家的未来角色：判断力与责任感

那么，数学的未来将走向何方？假设目前AI系统的所有问题（如引用缺失和幻觉）都能被修复，数学家又将扮演什么角色？

首先也是最重要的一点，数学家的核心作用是**判断力**：决定提出什么问题、证明什么定理、编写什么算法。这需要经验。

作为一名应用数学家，我的主要职责是将利益相关者模糊的问题转化为具体的数学问题。AI或许能解决一个给定的数学问题，但它没有解决问题的欲望，没有创造性洞察力的驱动，对问题是否有意义没有意见，对正确的切入点也没有立场。相比之下，人类合作者拥有自己的观点，能够辩论我们是否提出了正确的问题，能够激发截然不同的方法，甚至改变我整个数学视角。

未来的数学家无疑会将AI系统作为强大的工具——就像他们曾经接纳计算机和互联网一样——但他们不会被AI取代。我希望未来的数学研究，是由那些真正关心工作成果、对结果的正确性负责，并愿意付出艰辛努力（包括仔细审查AI输出结果）的人来完成的。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/数学]]
- [[00-元语/evals]]
- [[00-元语/benchmark]]
- [[00-元语/llm]]
- [[00-元语/risk]]
