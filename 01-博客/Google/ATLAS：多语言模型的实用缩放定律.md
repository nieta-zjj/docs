---
title: "ATLAS：多语言模型的实用缩放定律"
---

## 摘要

**1) 一句话总结**
本文介绍了ATLAS（自适应迁移缩放定律），这是一项基于迄今最大规模多语言预训练研究的实用指南，旨在帮助开发者在构建多语言AI模型时，基于数据驱动高效优化模型规模、数据量和语言混合比例。

**2) 关键要点**
*   **研究规模空前**：该研究涵盖了1000万至80亿参数模型间的774次训练运行，使用了400多种语言的数据，并在48种语言及1400对语言之间进行了评估。
*   **ATLAS三大核心组件**：包含跨语言迁移矩阵（确定最佳共训语言）、多语言缩放定律（指导模型和数据扩展）以及决策规则（选择预训练或微调）。
*   **精准的数据源建模**：ATLAS将数据分为目标语言、相似迁移语言和其他语言三类，以量化每种数据源对目标语言性能的实际影响。
*   **跨语言迁移规律**：共享书写系统或语系（如拉丁字母）是产生积极迁移的最大预测因素；英语、法语和西班牙语对其他语言的帮助最广泛，且语言间的迁移往往是不对称的。
*   **破解“多语言诅咒”的量化规则**：当模型支持的语言数量翻倍（2K）时，最佳策略是将模型规模增加1.18倍，总数据量增加1.66倍，利用语言间的积极协同效应来抵消容量限制。
*   **预训练与微调的交叉点**：对于20亿参数的模型，从头预训练与从多语言检查点微调的性能交叉点通常出现在1440亿到2830亿个Token之间。
*   **基于预算的训练决策**：如果计算和Token预算低于上述交叉点，建议从多语言检查点进行微调；如果预算充足，从头开始预训练通常能获得更好的最终结果。

**3) 风险与缺口**
*   **计算效率“税”**：使用多语言词表或完全多语言数据进行训练会带来计算效率上的折损（尤其是对英语而言）。
*   **低资源语言的数据耗尽瓶颈**：资源匮乏的语言在数据耗尽时，模型难以从重复数据中继续有效学习（表现为缩放曲线向上弯曲）。
*   **“多语言诅咒”的容量限制**：由于模型容量有限，在多种语言上训练的模型随着新语言的不断加入，可能会面临性能下降的风险。

## 正文

超过50%的AI模型用户使用非英语语言，但目前公开的缩放定律（Scaling Laws）绝大多数都集中在英语上。这种不平衡造成了公共研究的严重脱节，使得开发者在为数十亿国际用户构建非英语或特定语言混合模型时，缺乏关于效率、质量和成本的数据驱动指导。

为了填补这一空白，我们推出了迄今为止最大规模的公开多语言预训练研究，涵盖了1000万至80亿参数模型之间的774次训练运行，包含400多种语言的数据，并在48种语言上进行了评估。基于此研究，我们评估了1400对语言之间的协同效应，并引入了自适应迁移缩放定律（ATLAS），帮助从业者在训练数据中的语言混合比例与模型规模之间实现高效平衡。

### 适应多语言混合的单一缩放定律

ATLAS是一种简单实用的方法，用于确定最佳的模型规模、数据量和训练语言混合比例。与专注于单语环境的传统缩放定律不同，ATLAS为更复杂的多语言环境提供了指导。它通过利用多种不同语言的数据，专门优化目标语言（例如加泰罗尼亚语）的性能。

ATLAS通过以下三个组件扩展了传统的缩放定律原则：
*   **跨语言迁移矩阵**：用于确定哪些语言最适合一起训练。
*   **多语言缩放定律**：提供随着支持语言数量增加，如何高效扩展模型规模和数据量的指导。
*   **决策规则**：用于决定何时从头开始预训练模型，何时从多语言检查点进行微调。

ATLAS通过在数百个多语言实验上进行训练，并考虑三种不同的数据源来实现这一点：1）目标语言；2）根据经验分析得出的相似迁移语言（例如，加泰罗尼亚语可能包括西班牙语、葡萄牙语和意大利语等拉丁语系）；3）所有其他语言。这种新颖的方法使得该定律能够学习每种数据源对目标语言的实际帮助或阻碍程度，这是以往的定律所不具备的能力。

### 评估与发现

我们评估了ATLAS在预测模型在不同规模、不同训练数据量或新语言混合情况下的性能表现。评估结果表明，ATLAS始终优于以往的研究。

我们分析了ATLAS对六种语言（英语、法语、俄语、中文、印地语和斯瓦希里语）最佳模型规模和数据规模扩展轨迹的预测。比较这些语言的最佳缩放曲线时，我们发现了两个现象：
1. 各语言的曲线看起来惊人地相似。
2. 使用多语言词表或完全多语言数据进行训练会带来计算效率上的“税”（尤其是对英语而言）。资源匮乏的语言在数据耗尽时，曲线会出现向上弯曲，模型难以从重复数据中学习。ATLAS明确地对这些效应进行了建模。

### 跨语言迁移图谱

我们大规模测量了语言间的协同与干扰效应，生成了一个量化矩阵，展示了训练语言A对语言B的帮助（或损害）程度。结果非常符合直觉：挪威语主要受益于瑞典语和德语，马来语受益于印尼语，阿拉伯语受益于希伯来语。英语、法语和西班牙语是最具广泛帮助作用的训练语言，这可能是由于网络上这些语言文本固有的质量、异质性和庞大数量。

分析表明，积极迁移的最大预测因素是共享书写系统和/或语系（例如拉丁字母），这在统计学上具有极高的显著性。英语能帮助许多语言，但并非全部；而且迁移并不总是对称的（A对B的帮助可能大于B对A的帮助）。这些测量结果将“直觉”转化为了数据驱动的语言混合选择。

### 破解“多语言诅咒”的明确缩放规则

“多语言诅咒”是指由于模型容量有限，在多种语言上训练的模型随着新语言的加入，性能会出现下降的现象。我们通过一个缩放定律将这一问题形式化，该定律不仅考虑模型规模（N）和训练数据量（D），还考虑数据中的语言数量（K）。

通过将该定律拟合到大量实验中，我们发现：虽然增加语言会带来轻微的容量负担，但也存在高度的积极迁移。这意味着，如果我们想训练一个支持两倍语言数量（2K）的模型，我们应该将模型规模增加1.18倍，总数据量增加1.66倍。这相当于2K种语言中每种语言的数据量占原来的83%。尽管每种语言的数据量减少了，但从所有语言中学习带来的积极协同效应抵消了导致性能下降的容量限制。

### 何时预训练 vs. 微调多语言检查点

对于十种语言，我们比较了获得最佳性能模型的两条路径：(a) 在目标语言上从头开始预训练；(b) 从一个强大的多语言检查点进行微调。

从多语言检查点微调通常能以最少的额外计算量获得最佳性能，因为模型在跨语言方面已经非常强大。然而，如果模型可以训练更长时间，从头预训练通常能产生更好的长期结果。我们的目标是根据开发者可用的计算资源，找到两条训练曲线之间的交叉点。

结果显示，微调在早期占据优势，但当你有足够的Token时，预训练会反超。在我们的运行中，对于20亿参数的模型，交叉点通常出现在大约1440亿到2830亿个Token之间（具体取决于语言）。基于模型规模，我们得出了一个考虑预算的经验法则：如果你的Token和计算预算低于该模型规模的交叉点，请从多语言检查点开始；否则，从头开始预训练通常会取得更好的最终结果。

### 实践指南

ATLAS超越了以英语为中心的缩放定律，为全球模型开发者提供了一份路线图。它可以直接应用于将语言模型扩展到英语之外，帮助开发者解决以下问题：

*   **计划训练新的多语言或非英语模型？** 可以利用ATLAS根据词汇或训练选择来预估潜在的缩放定律。
*   **选择新的训练混合比例？** 参考迁移矩阵，选择能从经验上帮助目标语言的源语言——尤其是那些共享相同书写系统或语系的语言。
*   **训练包含更多语言的新模型？** 确定如何最有效地扩展模型规模和数据量，以减轻“多语言诅咒”的影响。
*   **计算资源受限？** 根据预算决定是微调多语言模型还是从头开始预训练。

我们希望这项工作能够推动新一代多语言模型的发展，更好地服务于数十亿非英语使用者。

## 关联主题

- [[00-元语/AI]]
- [[00-元语/llm]]
- [[00-元语/benchmark]]
- [[00-元语/paper]]
